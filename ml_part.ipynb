{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+--------------------------+--------------------+--------------------------+-----------------------+-----------------------+------------+\n",
      "| src_subnet_hashed|dst_subnet_vec|scaled_bytes_pkts_features|scaled_flow_duration|scaled_throughput_features|scaled_pkt_len_features|scaled_tcp_win_features|attack_index|\n",
      "+------------------+--------------+--------------------------+--------------------+--------------------------+-----------------------+-----------------------+------------+\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0513064471342...|[-0.4789529772200...|      [-0.0125662332574...|   [-0.4592399504915...|   [-0.4787111328395...|         1.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0513064471342...|[-0.4789529772200...|      [-0.0125662332574...|   [-0.4592399504915...|   [-0.4787111328395...|         1.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0513064471342...|[-0.4789529772200...|      [-0.0125662332574...|   [-0.4592399504915...|   [-0.4787111328395...|         1.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0275952189603...|  [2.08791390196378]|      [-0.0125633160103...|   [-0.4341342962160...|   [1.38149035044628...|         3.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0108895809287...|[-0.4789529772200...|      [-0.0125612606770...|   [0.00103037789422...|   [1.38149035044628...|         4.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0494742158662...|[-0.4789529772200...|      [-0.0125660078338...|   [-0.4236736069345...|   [-0.7822239306223...|         2.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0491508809365...|[-0.4789529772200...|      [-0.0125659680531...|   [-0.3504487819640...|   [-0.7822239306223...|         2.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0464564231895...|[-0.4789529772200...|      [-0.0125656365478...|   [-0.1893541670289...|   [-0.7822239306223...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0488275460069...|[-0.4789529772200...|      [-0.0125659282725...|   [-0.4111207797967...|   [-0.7822239306223...|         5.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0513064471342...|[-0.4789529772200...|      [-0.0125662332574...|   [-0.4592399504915...|   [-0.4787111328395...|         1.0|\n",
      "|(1024,[906],[1.0])|(94,[4],[1.0])|      [0.00743273175108...|[-0.4789529772200...|      [-0.0125590064406...|   [0.22907340423083...|   [1.38149035044628...|         6.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0039917690963...|[-0.4789529772200...|      [-0.0125604120233...|   [2.57854421685481...|   [1.38149035044628...|         4.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0513064471342...|[-0.4789529772200...|      [-0.0125662332574...|   [-0.4592399504915...|   [-0.4787111328395...|         1.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [0.37463343351662...|[-0.4789529772200...|      [-0.0125138288903...|   [-0.4759770533419...|   [-0.6606261520184...|         5.0|\n",
      "+------------------+--------------+--------------------------+--------------------+--------------------------+-----------------------+-----------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import parquet file with pySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session\n",
    "spark = SparkSession.builder.appName(\"parquet\").getOrCreate()\n",
    "\n",
    "# Read parquet file\n",
    "df = spark.read.parquet(\"ml_data_train.parquet\")\n",
    "\n",
    "# Show the data\n",
    "df.show()\n",
    "df.printSchema()\n",
    "print(\"Number of rows: \", df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scalling\n",
    "Feature scaling can play a crucial role in optimizing the performance of machine learning models, especially those sensitive to the scale of input data.\n",
    "Applying feature scaling to the right variables essential for improving model accuracy and efficiency. It ensures that each feature contributes equally to the decision-making process, preventing models from misinterpreting the data due to arbitrary feature scales. This leads to better, more reliable predictions in multiclass classification tasks.\n",
    "\n",
    "##### Numerical Variables with Different Scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Preparar os dados com VectorAssembler\n",
    "feature_columns = ['scaled_bytes_pkts_features', 'scaled_flow_duration', 'scaled_throughput_features', 'scaled_pkt_len_features', 'scaled_tcp_win_features']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, NaiveBayes, LogisticRegression, MultilayerPerceptronClassifier\n",
    "\n",
    "# Definir o Random Forest\n",
    "rf = RandomForestClassifier(labelCol=\"attack_index\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# Definir o Naive Bayes\n",
    "#nb = NaiveBayes(labelCol=\"attack_index\", featuresCol=\"features\")\n",
    "\n",
    "# Definir o Logistic Regression\n",
    "lr = LogisticRegression(labelCol=\"attack_index\", featuresCol=\"features\")\n",
    "\n",
    "# Configuração do Multilayer Perceptron\n",
    "# Obter o número de características\n",
    "def get_feature_count(df, feature_col=\"features\"):\n",
    "    # Extrai os metadados da coluna de características e calcula a soma dos tamanhos dos atributos\n",
    "    attributes = df.schema[feature_col].metadata[\"ml_attr\"][\"attrs\"]\n",
    "    feature_count = sum(len(attrs) for attrs in attributes.values())\n",
    "    return feature_count\n",
    "input_layers = get_feature_count(df_assembled, \"features\")\n",
    "output_layers = df_assembled.select(\"attack_index\").distinct().count()\n",
    "hidden_layers = [input_layers, (input_layers + output_layers) // 2, output_layers]\n",
    "mlp = MultilayerPerceptronClassifier(labelCol=\"attack_index\", featuresCol=\"features\", layers=hidden_layers, maxIter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento dos modelos\n",
    "model_rf = rf.fit(df_assembled)\n",
    "#model_nb = nb.fit(df_assembled)\n",
    "model_lr = lr.fit(df_assembled)\n",
    "model_mlp = mlp.fit(df_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluator per l'accuratezza\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"attack_index\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Evaluator per precision\n",
    "precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"attack_index\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "\n",
    "# Evaluator per recall\n",
    "recall_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"attack_index\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "# Evaluator per f1 score\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"attack_index\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8434\n",
      "Random Forest Precision: 0.8243855536746714\n",
      "Random Forest Recall: 0.8433999999999999\n",
      "Random Forest F1 Score: 0.8250611399869014\n"
     ]
    }
   ],
   "source": [
    "accuracy_rf = accuracy_evaluator.evaluate(model_rf.transform(df_assembled))\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf}\")\n",
    "precision_rf = precision_evaluator.evaluate(model_rf.transform(df_assembled))\n",
    "print(f\"Random Forest Precision: {precision_rf}\")\n",
    "recall_rf = recall_evaluator.evaluate(model_rf.transform(df_assembled))\n",
    "print(f\"Random Forest Recall: {recall_rf}\")\n",
    "f1_rf = f1_evaluator.evaluate(model_rf.transform(df_assembled))\n",
    "print(f\"Random Forest F1 Score: {f1_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6209\n",
      "Logistic Regression Precision: 0.6591741624901554\n",
      "Logistic Regression Recall: 0.6209\n",
      "Logistic Regression F1 Score: 0.5584675281195629\n"
     ]
    }
   ],
   "source": [
    "accuracy_lr = accuracy_evaluator.evaluate(model_lr.transform(df_assembled))\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_lr}\")\n",
    "precision_lr = precision_evaluator.evaluate(model_lr.transform(df_assembled))\n",
    "print(f\"Logistic Regression Precision: {precision_lr}\")\n",
    "recall_lr = recall_evaluator.evaluate(model_lr.transform(df_assembled))\n",
    "print(f\"Logistic Regression Recall: {recall_lr}\")\n",
    "f1_lr = f1_evaluator.evaluate(model_lr.transform(df_assembled))\n",
    "print(f\"Logistic Regression F1 Score: {f1_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilayer Perceptron Accuracy: 0.8147\n",
      "Multilayer Perceptron Precision: 0.8152272248573532\n",
      "Multilayer Perceptron Recall: 0.8147000000000001\n",
      "Multilayer Perceptron F1 Score: 0.8039290252069263\n"
     ]
    }
   ],
   "source": [
    "accuracy_mlp = accuracy_evaluator.evaluate(model_mlp.transform(df_assembled))\n",
    "print(f\"Multilayer Perceptron Accuracy: {accuracy_mlp}\")\n",
    "precision_mlp = precision_evaluator.evaluate(model_mlp.transform(df_assembled))\n",
    "print(f\"Multilayer Perceptron Precision: {precision_mlp}\")\n",
    "recall_mlp = recall_evaluator.evaluate(model_mlp.transform(df_assembled))\n",
    "print(f\"Multilayer Perceptron Recall: {recall_mlp}\")\n",
    "f1_mlp = f1_evaluator.evaluate(model_mlp.transform(df_assembled))\n",
    "print(f\"Multilayer Perceptron F1 Score: {f1_mlp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "vscode_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
