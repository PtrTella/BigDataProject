{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+--------------------------+--------------------+--------------------------+-----------------------+-----------------------+------------+\n",
      "| src_subnet_hashed|dst_subnet_vec|scaled_bytes_pkts_features|scaled_flow_duration|scaled_throughput_features|scaled_pkt_len_features|scaled_tcp_win_features|attack_index|\n",
      "+------------------+--------------+--------------------------+--------------------+--------------------------+-----------------------+-----------------------+------------+\n",
      "|(1024,[906],[1.0])| (6,[0],[1.0])|      [-0.3340954290867...|[-0.5099706138072...|      [-0.0734753712719...|   [-0.4919874575015...|   [-0.5363250954674...|         1.0|\n",
      "|(1024,[906],[1.0])| (6,[0],[1.0])|      [-0.3340954290867...|[-0.5099706138072...|      [-0.0734753712719...|   [-0.4919874575015...|   [-0.5363250954674...|         1.0|\n",
      "|(1024,[906],[1.0])| (6,[0],[1.0])|      [-0.3375790522617...|[-0.5099706138072...|      [-0.0734771216598...|   [-0.4999859529900...|   [-0.7600385009822...|         0.0|\n",
      "|(1024,[906],[1.0])| (6,[0],[1.0])|      [-0.3340954290867...|[-0.5099706138072...|      [-0.0734753712719...|   [-0.4919874575015...|   [-0.5363250954674...|         1.0|\n",
      "|(1024,[906],[1.0])| (6,[0],[1.0])|      [-0.1424961544628...|[1.9577713684699747]|      [-0.0733790999356...|   [-0.4679919710358...|   [1.29183289022437...|         3.0|\n",
      "|(1024,[906],[1.0])| (6,[0],[1.0])|      [-0.3375790522617...|[-0.5099706138072...|      [-0.0734771216598...|   [-0.4999859529900...|   [-0.7600385009822...|         0.0|\n",
      "|(1024,[906],[1.0])| (6,[0],[1.0])|      [-0.3375790522617...|[-0.5099706138072...|      [-0.0734771216598...|   [-0.4999859529900...|   [-0.7600385009822...|         0.0|\n",
      "|(1024,[906],[1.0])| (6,[0],[1.0])|      [-0.0075057564324...|[-0.5099706138072...|      [-0.0733112724032...|   [-0.0520702056305...|   [1.29183289022437...|         4.0|\n",
      "|(1024,[906],[1.0])| (6,[0],[1.0])|      [-0.3192900305930...|[-0.5099706138072...|      [-0.0734679321231...|   [-0.4579938516751...|   [-0.8346096361538...|         2.0|\n",
      "|(1024,[906],[1.0])| (6,[0],[1.0])|      [-0.3375790522617...|[-0.5099706138072...|      [-0.0734771216598...|   [-0.4999859529900...|   [-0.7600385009822...|         0.0|\n",
      "+------------------+--------------+--------------------------+--------------------+--------------------------+-----------------------+-----------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import parquet file with pySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session\n",
    "spark = SparkSession.builder.appName(\"parquet\").getOrCreate()\n",
    "\n",
    "# Read parquet file\n",
    "df = spark.read.parquet(\"ml_data_train.parquet\")\n",
    "\n",
    "# Show the data\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scalling\n",
    "Feature scaling can play a crucial role in optimizing the performance of machine learning models, especially those sensitive to the scale of input data.\n",
    "Applying feature scaling to the right variables essential for improving model accuracy and efficiency. It ensures that each feature contributes equally to the decision-making process, preventing models from misinterpreting the data due to arbitrary feature scales. This leads to better, more reliable predictions in multiclass classification tasks.\n",
    "\n",
    "##### Numerical Variables with Different Scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics summary for numerical variables candidates for scalling\n",
    "columns_of_interest_for_scaling = [\n",
    "    \"IN_BYTES\", \"OUT_BYTES\", \"IN_PKTS\", \"OUT_PKTS\", \n",
    "    \"FLOW_DURATION_MILLISECONDS\", \"SRC_TO_DST_SECOND_BYTES\", \n",
    "    \"DST_TO_SRC_SECOND_BYTES\", \"LONGEST_FLOW_PKT\", \"SHORTEST_FLOW_PKT\", \n",
    "    \"MIN_IP_PKT_LEN\", \"MAX_IP_PKT_LEN\", \"TCP_WIN_MAX_IN\", \"TCP_WIN_MAX_OUT\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, FeatureHasher, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# OneHot encoding for destination subnet since higher cardinality\n",
    "dst_subnet_encoder = OneHotEncoder(inputCol=\"dst_subnet_index\", outputCol=\"dst_subnet_vec\")\n",
    "\n",
    "# Source address\n",
    "# Feature hashing for source subnet since lower cardinality\n",
    "src_subnet_hasher = FeatureHasher(inputCols=[\"src_subnet_index\"], outputCol=\"src_subnet_hashed\", numFeatures=1024)\n",
    "\n",
    "# Assemble numerical features into vectors\n",
    "bytes_pkts_assembler = VectorAssembler(\n",
    "    inputCols=[\"IN_BYTES\", \"OUT_BYTES\", \"IN_PKTS\", \"OUT_PKTS\"],\n",
    "    outputCol=\"bytes_pkts_features\"\n",
    ")\n",
    "\n",
    "flow_duration_assembler = VectorAssembler(\n",
    "    inputCols=[\"FLOW_DURATION_MILLISECONDS\"],\n",
    "    outputCol=\"flow_duration_feature\"\n",
    ")\n",
    "\n",
    "throughput_assembler = VectorAssembler(\n",
    "    inputCols=[\"SRC_TO_DST_SECOND_BYTES\", \"DST_TO_SRC_SECOND_BYTES\"],\n",
    "    outputCol=\"throughput_features\"\n",
    ")\n",
    "\n",
    "pkt_len_assembler = VectorAssembler(\n",
    "    inputCols=[\"LONGEST_FLOW_PKT\", \"SHORTEST_FLOW_PKT\", \"MIN_IP_PKT_LEN\", \"MAX_IP_PKT_LEN\"],\n",
    "    outputCol=\"pkt_len_features\"\n",
    ")\n",
    "\n",
    "tcp_win_assembler = VectorAssembler(\n",
    "    inputCols=[\"TCP_WIN_MAX_IN\", \"TCP_WIN_MAX_OUT\"],\n",
    "    outputCol=\"tcp_win_features\"\n",
    ")\n",
    "\n",
    "# Apply StandardScaler to the assembled vectors\n",
    "bytes_pkts_scaler = StandardScaler(\n",
    "    inputCol=\"bytes_pkts_features\", \n",
    "    outputCol=\"scaled_bytes_pkts_features\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "flow_duration_scaler = StandardScaler(\n",
    "    inputCol=\"flow_duration_feature\", \n",
    "    outputCol=\"scaled_flow_duration\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "throughput_scaler = StandardScaler(\n",
    "    inputCol=\"throughput_features\", \n",
    "    outputCol=\"scaled_throughput_features\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "pkt_len_scaler = StandardScaler(\n",
    "    inputCol=\"pkt_len_features\", \n",
    "    outputCol=\"scaled_pkt_len_features\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "tcp_win_scaler = StandardScaler(\n",
    "    inputCol=\"tcp_win_features\", \n",
    "    outputCol=\"scaled_tcp_win_features\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Create a pipeline for scaling\n",
    "scaling_pipeline = Pipeline(stages=[\n",
    "    dst_subnet_encoder,\n",
    "    src_subnet_hasher,\n",
    "    bytes_pkts_assembler, \n",
    "    bytes_pkts_scaler, \n",
    "    flow_duration_assembler, \n",
    "    flow_duration_scaler, \n",
    "    throughput_assembler, \n",
    "    throughput_scaler, \n",
    "    pkt_len_assembler, \n",
    "    pkt_len_scaler, \n",
    "    tcp_win_assembler, \n",
    "    tcp_win_scaler\n",
    "])\n",
    "\n",
    "# Fit the scalling pipeline to the encoded training data\n",
    "scalling_model = scaling_pipeline.fit(encoded_train_data)\n",
    "\n",
    "# Transform both encoded training and test data\n",
    "processed_train_data = scalling_model.transform(encoded_train_data)\n",
    "processed_test_data = scalling_model.transform(encoded_test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "vscode_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
