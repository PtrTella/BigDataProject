{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+--------------------------+--------------------+--------------------------+-----------------------+-----------------------+------------+\n",
      "| src_subnet_hashed|dst_subnet_vec|scaled_bytes_pkts_features|scaled_flow_duration|scaled_throughput_features|scaled_pkt_len_features|scaled_tcp_win_features|attack_index|\n",
      "+------------------+--------------+--------------------------+--------------------+--------------------------+-----------------------+-----------------------+------------+\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0513064471342...|[-0.4789529772200...|      [-0.0125662332574...|   [-0.4592399504915...|   [-0.4787111328395...|         1.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0513064471342...|[-0.4789529772200...|      [-0.0125662332574...|   [-0.4592399504915...|   [-0.4787111328395...|         1.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0513064471342...|[-0.4789529772200...|      [-0.0125662332574...|   [-0.4592399504915...|   [-0.4787111328395...|         1.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0275952189603...|  [2.08791390196378]|      [-0.0125633160103...|   [-0.4341342962160...|   [1.38149035044628...|         3.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0108895809287...|[-0.4789529772200...|      [-0.0125612606770...|   [0.00103037789422...|   [1.38149035044628...|         4.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0494742158662...|[-0.4789529772200...|      [-0.0125660078338...|   [-0.4236736069345...|   [-0.7822239306223...|         2.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0491508809365...|[-0.4789529772200...|      [-0.0125659680531...|   [-0.3504487819640...|   [-0.7822239306223...|         2.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0464564231895...|[-0.4789529772200...|      [-0.0125656365478...|   [-0.1893541670289...|   [-0.7822239306223...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0488275460069...|[-0.4789529772200...|      [-0.0125659282725...|   [-0.4111207797967...|   [-0.7822239306223...|         5.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0513064471342...|[-0.4789529772200...|      [-0.0125662332574...|   [-0.4592399504915...|   [-0.4787111328395...|         1.0|\n",
      "|(1024,[906],[1.0])|(94,[4],[1.0])|      [0.00743273175108...|[-0.4789529772200...|      [-0.0125590064406...|   [0.22907340423083...|   [1.38149035044628...|         6.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0039917690963...|[-0.4789529772200...|      [-0.0125604120233...|   [2.57854421685481...|   [1.38149035044628...|         4.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0513064471342...|[-0.4789529772200...|      [-0.0125662332574...|   [-0.4592399504915...|   [-0.4787111328395...|         1.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [0.37463343351662...|[-0.4789529772200...|      [-0.0125138288903...|   [-0.4759770533419...|   [-0.6606261520184...|         5.0|\n",
      "+------------------+--------------+--------------------------+--------------------+--------------------------+-----------------------+-----------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import parquet file with pySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session\n",
    "spark = SparkSession.builder.appName(\"parquet\").getOrCreate()\n",
    "\n",
    "# Read parquet file\n",
    "df = spark.read.parquet(\"ml_data_train.parquet\")\n",
    "\n",
    "# Show the data\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scalling\n",
    "Feature scaling can play a crucial role in optimizing the performance of machine learning models, especially those sensitive to the scale of input data.\n",
    "Applying feature scaling to the right variables essential for improving model accuracy and efficiency. It ensures that each feature contributes equally to the decision-making process, preventing models from misinterpreting the data due to arbitrary feature scales. This leads to better, more reliable predictions in multiclass classification tasks.\n",
    "\n",
    "##### Numerical Variables with Different Scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Preparar os dados com VectorAssembler\n",
    "feature_columns = ['scaled_bytes_pkts_features', 'scaled_flow_duration', 'scaled_throughput_features', 'scaled_pkt_len_features', 'scaled_tcp_win_features']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, NaiveBayes, LogisticRegression, MultilayerPerceptronClassifier\n",
    "\n",
    "# Definir o Random Forest\n",
    "rf = RandomForestClassifier(labelCol=\"attack_index\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# Definir o Naive Bayes\n",
    "#nb = NaiveBayes(labelCol=\"attack_index\", featuresCol=\"features\")\n",
    "\n",
    "# Definir o Logistic Regression\n",
    "lr = LogisticRegression(labelCol=\"attack_index\", featuresCol=\"features\")\n",
    "\n",
    "# Configuração do Multilayer Perceptron\n",
    "# Obter o número de características\n",
    "def get_feature_count(df, feature_col=\"features\"):\n",
    "    # Extrai os metadados da coluna de características e calcula a soma dos tamanhos dos atributos\n",
    "    attributes = df.schema[feature_col].metadata[\"ml_attr\"][\"attrs\"]\n",
    "    feature_count = sum(len(attrs) for attrs in attributes.values())\n",
    "    return feature_count\n",
    "input_layers = get_feature_count(df_assembled, \"features\")\n",
    "output_layers = df_assembled.select(\"attack_index\").distinct().count()\n",
    "hidden_layers = [input_layers, (input_layers + output_layers) // 2, output_layers]\n",
    "mlp = MultilayerPerceptronClassifier(labelCol=\"attack_index\", featuresCol=\"features\", layers=hidden_layers, maxIter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento dos modelos\n",
    "model_rf = rf.fit(df_assembled)\n",
    "#model_nb = nb.fit(df_assembled)\n",
    "model_lr = lr.fit(df_assembled)\n",
    "model_mlp = mlp.fit(df_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8129\n",
      "Logistic Regression Accuracy: 0.6209\n",
      "Multilayer Perceptron Accuracy: 0.8125\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Avaliador\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"attack_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Avaliar Random Forest\n",
    "accuracy_rf = evaluator.evaluate(model_rf.transform(df_assembled))\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf}\")\n",
    "\n",
    "# Avaliar Naive Bayes\n",
    "#accuracy_nb = evaluator.evaluate(model_nb.transform(df_assembled))\n",
    "#print(f\"Naive Bayes Accuracy: {accuracy_nb}\")\n",
    "\n",
    "# Avaliar Logistic Regression\n",
    "accuracy_lr = evaluator.evaluate(model_lr.transform(df_assembled))\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_lr}\")\n",
    "\n",
    "# Avaliar Multilayer Perceptron\n",
    "accuracy_mlp = evaluator.evaluate(model_mlp.transform(df_assembled))\n",
    "print(f\"Multilayer Perceptron Accuracy: {accuracy_mlp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "vscode_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
