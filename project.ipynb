{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n"
     ]
    }
   ],
   "source": [
    "# check if pyspark works\n",
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Multiclass classification IoT\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "def load_test_data(debug=False):\n",
    "    if debug:\n",
    "        test_data = spark.read.csv(r\"./dataset/NF-ToN-IoT-v2-test.csv\", header=True, inferSchema=True).limit(200)\n",
    "        train_data = spark.read.csv(r\"./dataset/NF-ToN-IoT-v2-train.csv\", header=True, inferSchema=True).limit(300000)\n",
    "    else:\n",
    "        test_data = spark.read.csv(r\"./dataset/NF-ToN-IoT-v2-test.csv\", header=True, inferSchema=True)\n",
    "        train_data = spark.read.csv(r\"./dataset/NF-ToN-IoT-v2-train.csv\", header=True, inferSchema=True)\n",
    "        \n",
    "    return test_data, train_data\n",
    "\n",
    "test_data, train_data = load_test_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the training dataset to verify it's loaded correctly\n",
    "train_data.show(5)\n",
    "print(train_data.count())\n",
    "\n",
    "# Display the first few rows of the testing dataset to verify it's loaded correctly\n",
    "test_data.show(5)\n",
    "print(test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the schema of the loaded data to confirm the data types of each column\n",
    "train_data.printSchema()\n",
    "test_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying missing values\n",
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "\n",
    "# Count nulls and NaNs in each column\n",
    "def count_missing(data):\n",
    "    #data.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in data.columns]).show()\n",
    "    data.select([count(when(isnan(c), c)).alias(c) for c in data.columns]).show()\n",
    "\n",
    "count_missing(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying missing values\n",
    "#count_missing(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is clean and doesn't have any missing values, so there's no need for further cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding\n",
    "On the dataset, which consists of network traffic data, several categorical variables could potentially benefit from encoding. The decision to apply encoding techniques depends on whether the variables are nominal (without an inherent order) or ordinal (with a specific order) and whether they are used as features in the model. \n",
    "\n",
    "After an understanding of data and the variables meaning, we considered some variables for encoding:\n",
    "- **IPV4_SRC_ADDR and IPV4_DST_ADDR** (Categorical nominal): These are IP addresses and typically should be treated as categorical.\n",
    "- **Attack** (categorical Nominal): In machine learning projects involving classification tasks, the target variable (also known as the label or response variable) is crucial as it's the outcome the model is trying to predict. On the project, the target variable consists of categorical data (e.g., text labels representing different classes), it needs to be converted into a numeric format. This conversion is essential because most machine learning algorithms require numeric input to perform calculations during model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Count distinct values in IPV4_SRC_ADDR\n",
    "distinct_src_subnet_count = train_data.select(countDistinct(\"IPV4_SRC_ADDR\").alias(\"Distinct_SRC_Count\"))\n",
    "\n",
    "# Count distinct values in IPV4_DST_ADDR\n",
    "distinct_dst_subnet_count = train_data.select(countDistinct(\"IPV4_DST_ADDR\").alias(\"Distinct_DST_Subnet\"))\n",
    "\n",
    "# Show the results\n",
    "distinct_src_subnet_count.show(truncate=False)\n",
    "distinct_dst_subnet_count.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the high number of distinct values in the IPV4_ADDR column, subnet segmentation was performed to facilitate more effective encoding. This approach reduces the granularity of the data, thereby simplifying the feature space without significantly compromising the informational value of the IP addresses. Segmentation enables us to manage the high cardinality of the IP addresses, which is critical for applying machine learning techniques efficiently and effectively. By categorizing the IP addresses into their respective subnets, we can capture essential network-level behaviors while avoiding the computational complexity associated with the vast number of unique full IP addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, split, col\n",
    "\n",
    "def add_subnet_columns(data):\n",
    "    # Add a new column for the subnet (first two octets) for source and destination IP addresses\n",
    "    data = data.withColumn(\n",
    "        \"IPV4_SRC_ADDR_Subnet\",\n",
    "        concat_ws(\".\", split(col(\"IPV4_SRC_ADDR\"), \"\\\\.\")[0], split(col(\"IPV4_SRC_ADDR\"), \"\\\\.\")[1])\n",
    "    )\n",
    "\n",
    "    data = data.withColumn(\n",
    "        \"IPV4_DST_ADDR_Subnet\",\n",
    "        concat_ws(\".\", split(col(\"IPV4_DST_ADDR\"), \"\\\\.\")[0], split(col(\"IPV4_DST_ADDR\"), \"\\\\.\")[1])\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = add_subnet_columns(train_data)\n",
    "test_data = add_subnet_columns(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the new columns along with the original IP addresses\n",
    "train_data.select(\"IPV4_SRC_ADDR\", \"IPV4_SRC_ADDR_Subnet\", \"IPV4_DST_ADDR\", \"IPV4_DST_ADDR_Subnet\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Count distinct values in IPV4_SRC_ADDR_Subnet\n",
    "distinct_src_subnet_count = train_data.select(countDistinct(\"IPV4_SRC_ADDR_Subnet\").alias(\"Distinct_SRC_Subnet_Count\"))\n",
    "\n",
    "# Count distinct values in IPV4_DST_ADDR_Subnet\n",
    "distinct_dst_subnet_count = train_data.select(countDistinct(\"IPV4_DST_ADDR_Subnet\").alias(\"Distinct_DST_Subnet_Count\"))\n",
    "\n",
    "# Show the results\n",
    "distinct_src_subnet_count.show(truncate=False)\n",
    "distinct_dst_subnet_count.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Source IP Dimensionality:** The modest reduction in unique source subnets compared to the full source IP count suggests that the network traffic originates from a diverse set of locations or devices, with slightly clustered but still quite spread out origins. This might imply that any predictive modeling using source subnets as a feature would still need to handle a relatively high number of categories, potentially requiring further techniques to manage dimensionality or enhance interpretability.\n",
    "\n",
    "- **Destination IP Dimensionality:** The more significant reduction in unique destination subnets points to a higher level of concentration of network traffic towards certain destination networks or servers. For modeling purposes, this could mean that destination subnet could be a more impactful feature, providing stronger predictive signals with fewer categories, thus improving model performance and simplicity.\n",
    "\n",
    "Given the results, for destination subnets, straightforward categorical encoding methods like one-hot encoding might be feasible given the reduced number of unique values. For source subnets, considering the still high number of unique categories, methods like feature hashing or embedding might be more appropriate to prevent models from becoming too complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+--------------------+--------------------+----------------+\n",
      "|Attack   |attack_index|IPV4_SRC_ADDR_Subnet|IPV4_DST_ADDR_Subnet|dst_subnet_index|\n",
      "+---------+------------+--------------------+--------------------+----------------+\n",
      "|scanning |1.0         |192.168             |192.168             |0.0             |\n",
      "|scanning |1.0         |192.168             |192.168             |0.0             |\n",
      "|Benign   |0.0         |192.168             |192.168             |0.0             |\n",
      "|scanning |1.0         |192.168             |192.168             |0.0             |\n",
      "|ddos     |3.0         |192.168             |192.168             |0.0             |\n",
      "|Benign   |0.0         |192.168             |192.168             |0.0             |\n",
      "|Benign   |0.0         |192.168             |192.168             |0.0             |\n",
      "|password |4.0         |192.168             |192.168             |0.0             |\n",
      "|xss      |2.0         |192.168             |192.168             |0.0             |\n",
      "|Benign   |0.0         |192.168             |192.168             |0.0             |\n",
      "|xss      |2.0         |192.168             |192.168             |0.0             |\n",
      "|Benign   |0.0         |192.168             |192.168             |0.0             |\n",
      "|dos      |5.0         |192.168             |192.168             |0.0             |\n",
      "|scanning |1.0         |192.168             |192.168             |0.0             |\n",
      "|injection|6.0         |192.168             |18.184              |5.0             |\n",
      "|password |4.0         |192.168             |192.168             |0.0             |\n",
      "|Benign   |0.0         |192.168             |192.168             |0.0             |\n",
      "|scanning |1.0         |192.168             |192.168             |0.0             |\n",
      "|Benign   |0.0         |192.168             |192.168             |0.0             |\n",
      "|dos      |5.0         |192.168             |192.168             |0.0             |\n",
      "+---------+------------+--------------------+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, FeatureHasher\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define Indexers and Encoders for categorical variables\n",
    "attack_indexer = StringIndexer(inputCol=\"Attack\", outputCol=\"attack_index\")\n",
    "\n",
    "# Destination address\n",
    "dst_subnet_indexer = StringIndexer(inputCol=\"IPV4_DST_ADDR_Subnet\", outputCol=\"dst_subnet_index\")\n",
    "\n",
    "# Source address\n",
    "# Feature hashing for source subnet since lower cardinality\n",
    "# Just for graphing\n",
    "src_subnet_indexer = StringIndexer(inputCol=\"IPV4_SRC_ADDR_Subnet\", outputCol=\"src_subnet_index\")\n",
    "\n",
    "\n",
    "# Create a pipeline for encoding\n",
    "encoding_pipeline = Pipeline(stages=[\n",
    "    attack_indexer, \n",
    "    dst_subnet_indexer, \n",
    "    src_subnet_indexer,\n",
    "])\n",
    "\n",
    "# Fit the encoding pipeline to the training data\n",
    "encoding_model = encoding_pipeline.fit(train_data)\n",
    "\n",
    "# Transform both training and test data\n",
    "encoded_train_data = encoding_model.transform(train_data)\n",
    "encoded_test_data = encoding_model.transform(test_data)\n",
    "\n",
    "# Show encoded features to verify\n",
    "encoded_train_data.select(\"Attack\", \"attack_index\", \"IPV4_SRC_ADDR_Subnet\", \"IPV4_DST_ADDR_Subnet\", \"dst_subnet_index\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L4_SRC_PORT',\n",
       " 'L4_DST_PORT',\n",
       " 'PROTOCOL',\n",
       " 'L7_PROTO',\n",
       " 'IN_BYTES',\n",
       " 'IN_PKTS',\n",
       " 'OUT_BYTES',\n",
       " 'OUT_PKTS',\n",
       " 'TCP_FLAGS',\n",
       " 'CLIENT_TCP_FLAGS',\n",
       " 'SERVER_TCP_FLAGS',\n",
       " 'FLOW_DURATION_MILLISECONDS',\n",
       " 'DURATION_IN',\n",
       " 'DURATION_OUT',\n",
       " 'MIN_TTL',\n",
       " 'MAX_TTL',\n",
       " 'LONGEST_FLOW_PKT',\n",
       " 'SHORTEST_FLOW_PKT',\n",
       " 'MIN_IP_PKT_LEN',\n",
       " 'MAX_IP_PKT_LEN',\n",
       " 'SRC_TO_DST_SECOND_BYTES',\n",
       " 'DST_TO_SRC_SECOND_BYTES',\n",
       " 'RETRANSMITTED_IN_BYTES',\n",
       " 'RETRANSMITTED_IN_PKTS',\n",
       " 'RETRANSMITTED_OUT_BYTES',\n",
       " 'RETRANSMITTED_OUT_PKTS',\n",
       " 'SRC_TO_DST_AVG_THROUGHPUT',\n",
       " 'DST_TO_SRC_AVG_THROUGHPUT',\n",
       " 'NUM_PKTS_UP_TO_128_BYTES',\n",
       " 'NUM_PKTS_128_TO_256_BYTES',\n",
       " 'NUM_PKTS_256_TO_512_BYTES',\n",
       " 'NUM_PKTS_512_TO_1024_BYTES',\n",
       " 'NUM_PKTS_1024_TO_1514_BYTES',\n",
       " 'TCP_WIN_MAX_IN',\n",
       " 'TCP_WIN_MAX_OUT',\n",
       " 'ICMP_TYPE',\n",
       " 'ICMP_IPV4_TYPE',\n",
       " 'DNS_QUERY_ID',\n",
       " 'DNS_QUERY_TYPE',\n",
       " 'DNS_TTL_ANSWER',\n",
       " 'FTP_COMMAND_RET_CODE',\n",
       " 'attack_index',\n",
       " 'dst_subnet_index',\n",
       " 'src_subnet_index']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the original columns that have been encoded and hashed\n",
    "encoded_train_data = encoded_train_data.drop(\"Attack\", \"Label\", \"IPV4_SRC_ADDR_Subnet\", \"IPV4_DST_ADDR_Subnet\", \"IPV4_SRC_ADDR\", \"IPV4_DST_ADDR\")\n",
    "\n",
    "# Show the final columns in the training data\n",
    "encoded_train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA part\n",
    "Converting dataset to pandas for graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Row limit for converting to Pandas DataFrame for plotting (to avoid memory issues)\n",
    "ROW_LIMIT = 200000\n",
    "\n",
    "# Converte il DataFrame Spark in un DataFrame Pandas per l'analisi esplorativa\n",
    "df_pd = encoded_train_data.limit(ROW_LIMIT).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisi esplorativa dei dati utilizzando Pandas e Seaborn\n",
    "def features_hisogram(dataframe):\n",
    "    # Istogrammi delle caratteristiche\n",
    "    dataframe.hist(bins=20, figsize=(96, 48))\n",
    "    plt.suptitle(\"Histograms of Features\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "features_hisogram(df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(dataframe):\n",
    "    # Matrice di correlazione\n",
    "    correlation_matrix = dataframe.corr()\n",
    "    plt.figure(figsize=(48, 48))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title('Correlation Matrix', fontsize=20)\n",
    "    plt.show()\n",
    "    \n",
    "heatmap(df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequencias de categoria\n",
    "# This function calculates the frequency of each category in a given column\n",
    "# It use the full dataset to calculate the frequencies\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "def category_frequencies(data, column):\n",
    "    # Calcular a frequência das categorias na variável 'Attack'\n",
    "    frequency = data.groupBy(column).agg(count(\"*\").alias(\"frequency\")).orderBy(\"frequency\", ascending=False)\n",
    "    #attack_freq.show(truncate=False)\n",
    "    return frequency\n",
    "\n",
    "category_frequencies(encoded_train_data, \"attack_index\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequencias de categoria (plot)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# This function plots the top categories in a given column\n",
    "# It reduces the dataset to the top ROW_LIMIT categories before plotting\n",
    "\n",
    "def plot_category_frequencies(data, column):\n",
    "    data = category_frequencies(data, column).limit(ROW_LIMIT).toPandas()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(data[column], data['frequency'], color='blue')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Top {ROW_LIMIT} Frequency of {column} Categories')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "plot_category_frequencies(encoded_train_data, \"attack_index\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPONENTS = 3\n",
    "\n",
    "TARGET = \"attack_index\"\n",
    "\n",
    "FEATURES = [\n",
    "    \"L4_SRC_PORT\",\n",
    "    \"L4_DST_PORT\",\n",
    "    \"ICMP_TYPE\",\n",
    "    \"ICMP_IPV4_TYPE\",\n",
    "    \"DNS_QUERY_ID\",\n",
    "    \"DNS_QUERY_TYPE\",\n",
    "    \"DNS_TTL_ANSWER\",\n",
    "    \"FTP_COMMAND_RET_CODE\",\n",
    "    \"IN_BYTES\",\n",
    "    \"OUT_BYTES\",\n",
    "    \"IN_PKTS\",\n",
    "    \"OUT_PKTS\",\n",
    "    \"FLOW_DURATION_MILLISECONDS\",\n",
    "    \"PROTOCOL\",\n",
    "    \"SRC_TO_DST_SECOND_BYTES\",\n",
    "    \"DST_TO_SRC_SECOND_BYTES\",\n",
    "    \"LONGEST_FLOW_PKT\",\n",
    "    \"SHORTEST_FLOW_PKT\",\n",
    "    \"MIN_IP_PKT_LEN\",\n",
    "    \"MAX_IP_PKT_LEN\",\n",
    "    \"TCP_WIN_MAX_IN\",\n",
    "    \"TCP_WIN_MAX_OUT\",\n",
    "    \"dst_subnet_index\",\n",
    "    \"src_subnet_index\",\n",
    "    \"CLIENT_TCP_FLAGS\",\n",
    "    \"SERVER_TCP_FLAGS\",\n",
    "    \"DURATION_OUT\",\n",
    "    \"DURATION_IN\",\n",
    "    \"MIN_TTL\",\n",
    "    \"NUM_PKTS_UP_TO_128_BYTES\",\n",
    "    \"RETRANSMITTED_IN_BYTES\",\n",
    "    \"MAX_TTL\",\n",
    "    \"NUM_PKTS_128_TO_256_BYTES\",\n",
    "    \"DST_TO_SRC_AVG_THROUGHPUT\",\n",
    "    \"NUM_PKTS_512_TO_1024_BYTES\",\n",
    "    \"NUM_PKTS_256_TO_512_BYTES\",\n",
    "    \"RETRANSMITTED_OUT_BYTES\",\n",
    "    \"RETRANSMITTED_OUT_PKTS\",\n",
    "    \"RETRANSMITTED_IN_PKTS\",\n",
    "    \"SRC_TO_DST_AVG_THROUGHPUT\",\n",
    "    \"L7_PROTO\",\n",
    "    \"NUM_PKTS_1024_TO_1514_BYTES\",\n",
    "    \"TCP_FLAGS\",\n",
    "]\n",
    "\n",
    "# Define the features\n",
    "FEATURES = [\n",
    "    \"L4_SRC_PORT\", \"L4_DST_PORT\", \"ICMP_TYPE\", \"ICMP_IPV4_TYPE\", \"DNS_QUERY_ID\", \"DNS_QUERY_TYPE\",\n",
    "    \"DNS_TTL_ANSWER\", \"FTP_COMMAND_RET_CODE\", \"IN_BYTES\", \"OUT_BYTES\", \"IN_PKTS\", \"OUT_PKTS\",\n",
    "    \"FLOW_DURATION_MILLISECONDS\", \"PROTOCOL\", \"SRC_TO_DST_SECOND_BYTES\", \"DST_TO_SRC_SECOND_BYTES\",\n",
    "    \"LONGEST_FLOW_PKT\", \"SHORTEST_FLOW_PKT\", \"MIN_IP_PKT_LEN\", \"MAX_IP_PKT_LEN\", \"TCP_WIN_MAX_IN\",\n",
    "    \"TCP_WIN_MAX_OUT\", \"dst_subnet_index\", \"src_subnet_index\", \"CLIENT_TCP_FLAGS\", \"SERVER_TCP_FLAGS\",\n",
    "    \"DURATION_OUT\", \"DURATION_IN\", \"MIN_TTL\", \"NUM_PKTS_UP_TO_128_BYTES\", \"RETRANSMITTED_IN_BYTES\",\n",
    "    \"MAX_TTL\", \"NUM_PKTS_128_TO_256_BYTES\", \"DST_TO_SRC_AVG_THROUGHPUT\", \"NUM_PKTS_512_TO_1024_BYTES\",\n",
    "    \"NUM_PKTS_256_TO_512_BYTES\", \"RETRANSMITTED_OUT_BYTES\", \"RETRANSMITTED_OUT_PKTS\", \"RETRANSMITTED_IN_PKTS\",\n",
    "    \"SRC_TO_DST_AVG_THROUGHPUT\", \"L7_PROTO\", \"NUM_PKTS_1024_TO_1514_BYTES\", \"TCP_FLAGS\"\n",
    "]\n",
    "\n",
    "\n",
    "feature_groups = {\n",
    "    \"trasimitted_in\": [\"IN_BYTES\", \"IN_PKTS\"],\n",
    "    \"trasmitted_out\": [\"OUT_BYTES\", \"OUT_PKTS\"],\n",
    "    \"retransimtted_in\": [\"RETRANSMITTED_IN_BYTES\", \"RETRANSMITTED_IN_PKTS\"],\n",
    "    \"retransimtted_out\": [\"RETRANSMITTED_OUT_BYTES\", \"RETRANSMITTED_OUT_PKTS\"],\n",
    "    \"throughput_in\": [\"SRC_TO_DST_SECOND_BYTES\", \"SRC_TO_DST_AVG_THROUGHPUT\"],\n",
    "    \"throughput_out\": [\"DST_TO_SRC_AVG_THROUGHPUT\", \"DST_TO_SRC_SECOND_BYTES\"],\n",
    "    \"ttl\": [\"MIN_TTL\", \"MAX_TTL\"],\n",
    "    \"tcp_flags\": [\"CLIENT_TCP_FLAGS\", \"SERVER_TCP_FLAGS\", \"TCP_FLAGS\"],\n",
    "    \"icmp\": [\"ICMP_TYPE\", \"ICMP_IPV4_TYPE\"],\n",
    "    \"flow_max\": [\"LONGEST_FLOW_PKT\", \"MAX_IP_PKT_LEN\"],\n",
    "    \"flow_min\": [\"SHORTEST_FLOW_PKT\", \"MIN_IP_PKT_LEN\"],\n",
    "    \"flow_duration\": [\"FLOW_DURATION_MILLISECONDS\"],\n",
    "    \"tcp_in\": [\"TCP_WIN_MAX_IN\"],\n",
    "    \"tcp_out\": [\"TCP_WIN_MAX_OUT\"],\n",
    "    \"duration_in\": [\"DURATION_IN\"],\n",
    "    \"duration_out\": [\"DURATION_OUT\"],\n",
    "    \"protocol\": [\"PROTOCOL\"],\n",
    "    \"l4_src_port\": [\"L4_SRC_PORT\"],\n",
    "    \"l4_dst_port\": [\"L4_DST_PORT\"],\n",
    "    \"dns\": [\"DNS_QUERY_ID\", \"DNS_QUERY_TYPE\", \"DNS_TTL_ANSWER\"],\n",
    "    \"ftp\": [\"FTP_COMMAND_RET_CODE\"],\n",
    "    \"l7_proto\": [\"L7_PROTO\"],\n",
    "    \"num_packets_by_size\": [\"NUM_PKTS_UP_TO_128_BYTES\", \"NUM_PKTS_128_TO_256_BYTES\", \"NUM_PKTS_256_TO_512_BYTES\", \"NUM_PKTS_512_TO_1024_BYTES\", \"NUM_PKTS_1024_TO_1514_BYTES\"]\n",
    "}\n",
    "\n",
    "feature_groups = {\n",
    "    \"ttl\": [\"MIN_TTL\", \"MAX_TTL\"],\n",
    "    \"tcp_flags\": [\"CLIENT_TCP_FLAGS\", \"SERVER_TCP_FLAGS\", \"TCP_FLAGS\"],\n",
    "    \"flow\": [\"LONGEST_FLOW_PKT\", \"SHORTEST_FLOW_PKT\"],\n",
    "    \"duration\" : [\"FLOW_DURATION_MILLISECONDS\", \"DURATION_OUT\", \"DURATION_IN\"],\n",
    "    \"pkt_size\": [\"MIN_IP_PKT_LEN\", \"MAX_IP_PKT_LEN\"],\n",
    "    \"tcp_win\": [\"TCP_WIN_MAX_IN\", \"TCP_WIN_MAX_OUT\"],\n",
    "    \"throughput\": [\"SRC_TO_DST_SECOND_BYTES\", \"DST_TO_SRC_SECOND_BYTES\", \"SRC_TO_DST_AVG_THROUGHPUT\", \"DST_TO_SRC_AVG_THROUGHPUT\"],\n",
    "    \"protocol\": [\"PROTOCOL\"],\n",
    "    \"l4_dst_port\": [\"L4_DST_PORT\"],\n",
    "    \"dns\": [\"DNS_QUERY_ID\", \"DNS_QUERY_TYPE\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# OneHot encoding for destination subnet since higher cardinality\n",
    "dst_subnet_encoder = OneHotEncoder(inputCol=\"dst_subnet_index\", outputCol=\"dst_subnet_vec\")\n",
    "\n",
    "# Feature hashing for source subnet since lower cardinality\n",
    "src_subnet_hasher = FeatureHasher(inputCols=[\"src_subnet_index\"], outputCol=\"src_subnet_hashed\", numFeatures=1024)\n",
    "\n",
    "# Create assemblers and scalers\n",
    "assemblers_and_scalers = []\n",
    "\n",
    "def create_assembler_and_scaler(inputCols, outputCol):\n",
    "    assembler = VectorAssembler(inputCols=inputCols, outputCol=outputCol)\n",
    "    scaler = StandardScaler(inputCol=outputCol, outputCol=f\"scaled_{outputCol}\", withStd=True, withMean=True)\n",
    "    return assembler, scaler\n",
    "\n",
    "for group_name, features in feature_groups.items():\n",
    "    assembler, scaler = create_assembler_and_scaler(features, f\"{group_name}_features\")\n",
    "    assemblers_and_scalers.append(assembler)\n",
    "    assemblers_and_scalers.append(scaler)\n",
    "\n",
    "# Create the complete pipeline\n",
    "pipeline = Pipeline(stages=[dst_subnet_encoder, src_subnet_hasher] + assemblers_and_scalers)\n",
    "\n",
    "# Fit the pipeline and transform the data\n",
    "pipeline_model = pipeline.fit(encoded_train_data)\n",
    "processed_train_data = pipeline_model.transform(encoded_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attack_index', 'dst_subnet_vec', 'src_subnet_hashed', 'scaled_ttl_features', 'scaled_tcp_flags_features', 'scaled_flow_features', 'scaled_duration_features', 'scaled_pkt_size_features', 'scaled_tcp_win_features', 'scaled_throughput_features', 'scaled_protocol_features', 'scaled_l4_dst_port_features', 'scaled_dns_features']\n"
     ]
    }
   ],
   "source": [
    "# Assemble all scaled features into a single feature vector\n",
    "all_scaled_features = [f\"scaled_{group_name}_features\" for group_name in feature_groups.keys()]\n",
    "#all_assebled_features = [f\"{group_name}_features\" for group_name in feature_groups.keys()]\n",
    "all_scaled_features = all_scaled_features + [\"src_subnet_hashed\", \"dst_subnet_vec\"]\n",
    "\n",
    "dataset_features_to_keep = all_scaled_features + [\"attack_index\"]\n",
    "to_remove = [c for c in processed_train_data.columns if c not in dataset_features_to_keep]\n",
    "# Save the processed training data removing the intermediate columns\n",
    "for column in to_remove:\n",
    "    processed_train_data = processed_train_data.drop(column)\n",
    "# Show the final columns in the training data\n",
    "print(processed_train_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset into parquet format\n",
    "processed_train_data.write.mode(\"overwrite\").parquet(\"scaled_simplified_train_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Multiclass classification IoT\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "def load_test_data(debug=False):\n",
    "    if debug:\n",
    "        train_data = spark.read.parquet(\"scaled_simplified_train_data.parquet\").limit(300000)\n",
    "        \n",
    "    else:\n",
    "        train_data = spark.read.parquet(\"scaled_simplified_train_data.parquet\")\n",
    "        \n",
    "    return train_data\n",
    "\n",
    "processed_train_data = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- attack_index: double (nullable = true)\n",
      " |-- dst_subnet_vec: vector (nullable = true)\n",
      " |-- src_subnet_hashed: vector (nullable = true)\n",
      " |-- scaled_ttl_features: vector (nullable = true)\n",
      " |-- scaled_tcp_flags_features: vector (nullable = true)\n",
      " |-- scaled_flow_features: vector (nullable = true)\n",
      " |-- scaled_duration_features: vector (nullable = true)\n",
      " |-- scaled_pkt_size_features: vector (nullable = true)\n",
      " |-- scaled_tcp_win_features: vector (nullable = true)\n",
      " |-- scaled_throughput_features: vector (nullable = true)\n",
      " |-- scaled_protocol_features: vector (nullable = true)\n",
      " |-- scaled_l4_dst_port_features: vector (nullable = true)\n",
      " |-- scaled_dns_features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['attack_index',\n",
       " 'dst_subnet_vec',\n",
       " 'src_subnet_hashed',\n",
       " 'scaled_ttl_features',\n",
       " 'scaled_tcp_flags_features',\n",
       " 'scaled_flow_features',\n",
       " 'scaled_duration_features',\n",
       " 'scaled_pkt_size_features',\n",
       " 'scaled_tcp_win_features',\n",
       " 'scaled_throughput_features',\n",
       " 'scaled_protocol_features',\n",
       " 'scaled_l4_dst_port_features',\n",
       " 'scaled_dns_features']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train_data.printSchema()\n",
    "# List the columns in the processed training data\n",
    "processed_train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPONENTS = 3\n",
    "\n",
    "TARGET = \"attack_index\"\n",
    "\n",
    "FEATURES = [\n",
    "    \"L4_SRC_PORT\",\n",
    "    \"L4_DST_PORT\",\n",
    "    \"ICMP_TYPE\",\n",
    "    \"ICMP_IPV4_TYPE\",\n",
    "    \"DNS_QUERY_ID\",\n",
    "    \"DNS_QUERY_TYPE\",\n",
    "    \"DNS_TTL_ANSWER\",\n",
    "    \"FTP_COMMAND_RET_CODE\",\n",
    "    \"IN_BYTES\",\n",
    "    \"OUT_BYTES\",\n",
    "    \"IN_PKTS\",\n",
    "    \"OUT_PKTS\",\n",
    "    \"FLOW_DURATION_MILLISECONDS\",\n",
    "    \"PROTOCOL\",\n",
    "    \"SRC_TO_DST_SECOND_BYTES\",\n",
    "    \"DST_TO_SRC_SECOND_BYTES\",\n",
    "    \"LONGEST_FLOW_PKT\",\n",
    "    \"SHORTEST_FLOW_PKT\",\n",
    "    \"MIN_IP_PKT_LEN\",\n",
    "    \"MAX_IP_PKT_LEN\",\n",
    "    \"TCP_WIN_MAX_IN\",\n",
    "    \"TCP_WIN_MAX_OUT\",\n",
    "    \"dst_subnet_index\",\n",
    "    \"src_subnet_index\",\n",
    "    \"CLIENT_TCP_FLAGS\",\n",
    "    \"SERVER_TCP_FLAGS\",\n",
    "    \"DURATION_OUT\",\n",
    "    \"DURATION_IN\",\n",
    "    \"MIN_TTL\",\n",
    "    \"NUM_PKTS_UP_TO_128_BYTES\",\n",
    "    \"RETRANSMITTED_IN_BYTES\",\n",
    "    \"MAX_TTL\",\n",
    "    \"NUM_PKTS_128_TO_256_BYTES\",\n",
    "    \"DST_TO_SRC_AVG_THROUGHPUT\",\n",
    "    \"NUM_PKTS_512_TO_1024_BYTES\",\n",
    "    \"NUM_PKTS_256_TO_512_BYTES\",\n",
    "    \"RETRANSMITTED_OUT_BYTES\",\n",
    "    \"RETRANSMITTED_OUT_PKTS\",\n",
    "    \"RETRANSMITTED_IN_PKTS\",\n",
    "    \"SRC_TO_DST_AVG_THROUGHPUT\",\n",
    "    \"L7_PROTO\",\n",
    "    \"NUM_PKTS_1024_TO_1514_BYTES\",\n",
    "    \"TCP_FLAGS\",\n",
    "]\n",
    "\n",
    "# Define the features\n",
    "FEATURES = [\n",
    "    \"L4_SRC_PORT\", \"L4_DST_PORT\", \"ICMP_TYPE\", \"ICMP_IPV4_TYPE\", \"DNS_QUERY_ID\", \"DNS_QUERY_TYPE\",\n",
    "    \"DNS_TTL_ANSWER\", \"FTP_COMMAND_RET_CODE\", \"IN_BYTES\", \"OUT_BYTES\", \"IN_PKTS\", \"OUT_PKTS\",\n",
    "    \"FLOW_DURATION_MILLISECONDS\", \"PROTOCOL\", \"SRC_TO_DST_SECOND_BYTES\", \"DST_TO_SRC_SECOND_BYTES\",\n",
    "    \"LONGEST_FLOW_PKT\", \"SHORTEST_FLOW_PKT\", \"MIN_IP_PKT_LEN\", \"MAX_IP_PKT_LEN\", \"TCP_WIN_MAX_IN\",\n",
    "    \"TCP_WIN_MAX_OUT\", \"dst_subnet_index\", \"src_subnet_index\", \"CLIENT_TCP_FLAGS\", \"SERVER_TCP_FLAGS\",\n",
    "    \"DURATION_OUT\", \"DURATION_IN\", \"MIN_TTL\", \"NUM_PKTS_UP_TO_128_BYTES\", \"RETRANSMITTED_IN_BYTES\",\n",
    "    \"MAX_TTL\", \"NUM_PKTS_128_TO_256_BYTES\", \"DST_TO_SRC_AVG_THROUGHPUT\", \"NUM_PKTS_512_TO_1024_BYTES\",\n",
    "    \"NUM_PKTS_256_TO_512_BYTES\", \"RETRANSMITTED_OUT_BYTES\", \"RETRANSMITTED_OUT_PKTS\", \"RETRANSMITTED_IN_PKTS\",\n",
    "    \"SRC_TO_DST_AVG_THROUGHPUT\", \"L7_PROTO\", \"NUM_PKTS_1024_TO_1514_BYTES\", \"TCP_FLAGS\"\n",
    "]\n",
    "\n",
    "\n",
    "feature_groups = {\n",
    "    \"trasimitted_in\": [\"IN_BYTES\", \"IN_PKTS\"],\n",
    "    \"trasmitted_out\": [\"OUT_BYTES\", \"OUT_PKTS\"],\n",
    "    \"retransimtted_in\": [\"RETRANSMITTED_IN_BYTES\", \"RETRANSMITTED_IN_PKTS\"],\n",
    "    \"retransimtted_out\": [\"RETRANSMITTED_OUT_BYTES\", \"RETRANSMITTED_OUT_PKTS\"],\n",
    "    \"throughput_in\": [\"SRC_TO_DST_SECOND_BYTES\", \"SRC_TO_DST_AVG_THROUGHPUT\"],\n",
    "    \"throughput_out\": [\"DST_TO_SRC_AVG_THROUGHPUT\", \"DST_TO_SRC_SECOND_BYTES\"],\n",
    "    \"ttl\": [\"MIN_TTL\", \"MAX_TTL\"],\n",
    "    \"tcp_flags\": [\"CLIENT_TCP_FLAGS\", \"SERVER_TCP_FLAGS\", \"TCP_FLAGS\"],\n",
    "    \"icmp\": [\"ICMP_TYPE\", \"ICMP_IPV4_TYPE\"],\n",
    "    \"flow\" : [\"LONGEST_FLOW_PKT\", \"SHORTEST_FLOW_PKT\", \"MIN_IP_PKT_LEN\", \"MAX_IP_PKT_LEN\", \"FLOW_DURATION_MILLISECONDS\"],\n",
    "    \"tcp_win\": [\"TCP_WIN_MAX_IN\", \"TCP_WIN_MAX_OUT\"],\n",
    "    \"duration\": [\"DURATION_OUT\", \"DURATION_IN\"],\n",
    "    \"protocol\": [\"PROTOCOL\"],\n",
    "    \"l4_src_port\": [\"L4_SRC_PORT\"],\n",
    "    \"l4_dst_port\": [\"L4_DST_PORT\"],\n",
    "    \"dns\": [\"DNS_QUERY_ID\", \"DNS_QUERY_TYPE\", \"DNS_TTL_ANSWER\"],\n",
    "    \"ftp\": [\"FTP_COMMAND_RET_CODE\"],\n",
    "    \"num_packets_by_size\": [\"NUM_PKTS_UP_TO_128_BYTES\", \"NUM_PKTS_128_TO_256_BYTES\", \"NUM_PKTS_256_TO_512_BYTES\", \"NUM_PKTS_512_TO_1024_BYTES\", \"NUM_PKTS_1024_TO_1514_BYTES\"]\n",
    "}\n",
    "\n",
    "feature_groups = {\n",
    "    \"ttl\": [\"MIN_TTL\", \"MAX_TTL\"],\n",
    "    \"tcp_flags\": [\"CLIENT_TCP_FLAGS\", \"SERVER_TCP_FLAGS\", \"TCP_FLAGS\"],\n",
    "    \"flow\": [\"LONGEST_FLOW_PKT\", \"SHORTEST_FLOW_PKT\"],\n",
    "    \"duration\" : [\"FLOW_DURATION_MILLISECONDS\", \"DURATION_OUT\", \"DURATION_IN\"],\n",
    "    \"pkt_size\": [\"MIN_IP_PKT_LEN\", \"MAX_IP_PKT_LEN\"],\n",
    "    \"tcp_win\": [\"TCP_WIN_MAX_IN\", \"TCP_WIN_MAX_OUT\"],\n",
    "    \"throughput\": [\"SRC_TO_DST_SECOND_BYTES\", \"DST_TO_SRC_SECOND_BYTES\", \"SRC_TO_DST_AVG_THROUGHPUT\", \"DST_TO_SRC_AVG_THROUGHPUT\"],\n",
    "    \"protocol\": [\"PROTOCOL\"],\n",
    "    \"l4_dst_port\": [\"L4_DST_PORT\"],\n",
    "    \"dns\": [\"DNS_QUERY_ID\", \"DNS_QUERY_TYPE\"]\n",
    "}\n",
    "\n",
    "\n",
    "all_scaled_features = [f\"scaled_{group_name}_features\" for group_name in feature_groups.keys()]\n",
    "all_assebled_features = [f\"{group_name}_features\" for group_name in feature_groups.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "components = 2\n",
    "final_assembler = VectorAssembler(inputCols=all_scaled_features + [\"dst_subnet_vec\", \"src_subnet_hashed\"], outputCol=\"final_features\")\n",
    "pca = PCA(k=components, inputCol=\"final_features\", outputCol=\"pca_features\")\n",
    "pipeline = Pipeline(stages=[final_assembler, pca])\n",
    "\n",
    "\n",
    "# Assemble all scaled features into a single feature vector\n",
    "#pca_model, pca_result = perform_pca(processed_train_data, components=COMPONENTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "py4j.reflection.TypeUtil.isInstanceOf does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pca_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_train_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:181\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 181\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:173\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    167\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  An exception was thrown from the Python worker. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see the stack trace below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m c\u001b[38;5;241m.\u001b[39mgetMessage()\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PythonException(msg, stacktrace)\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnknownException\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstackTrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacktrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:65\u001b[0m, in \u001b[0;36mCapturedException.__init__\u001b[0;34m(self, desc, stackTrace, cause, origin)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstackTrace \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     61\u001b[0m     stackTrace\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stackTrace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (SparkContext\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mUtils\u001b[38;5;241m.\u001b[39mexceptionString(origin))\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin\u001b[38;5;241m.\u001b[39mgetCause() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m convert_exception(origin\u001b[38;5;241m.\u001b[39mgetCause())\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:145\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_instance_of(gw, e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava.lang.ArithmeticException\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArithmeticException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_instance_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjava.lang.UnsupportedOperationException\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UnsupportedOperationException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_instance_of(gw, e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava.lang.ArrayIndexOutOfBoundsException\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:464\u001b[0m, in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava_class must be a string, a JavaClass, or a JavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy4j\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreflection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTypeUtil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misInstanceOf\u001b[49m(\n\u001b[1;32m    465\u001b[0m     param, java_object)\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:1549\u001b[0m, in \u001b[0;36mJavaClass.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1546\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m get_return_value(\n\u001b[1;32m   1547\u001b[0m             answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn, name)\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m   1550\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: py4j.reflection.TypeUtil.isInstanceOf does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "pca_model = pipeline.fit(processed_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result = pca_model.transform(processed_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract principal components from vectors\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "\n",
    "# Show the principal components\n",
    "def extract_component(vector, index):\n",
    "    return float(vector[index])\n",
    "\n",
    "for i in range(COMPONENTS):\n",
    "    processed_train_data = pca_model.withColumn(f'PC{i+1}', udf(lambda vector: extract_component(vector, i), FloatType())(col('pca_features')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_pca_scatter(\n",
    "    pca_result, target: str = TARGET, num_components: int = COMPONENTS\n",
    "):\n",
    "\n",
    "    # Convert PCA results to Pandas DataFrame for visualization\n",
    "    pca_pd = pca_result.limit(ROW_LIMIT).toPandas()\n",
    "\n",
    "    # Extract principal components into separate columns\n",
    "    pca_columns = [f\"PC{i+1}\" for i in range(num_components)]\n",
    "    pca_components = pd.DataFrame(\n",
    "        pca_pd[\"pca_features\"].apply(lambda x: x[:num_components]).tolist(),\n",
    "        columns=pca_columns,\n",
    "        index=pca_pd.index,\n",
    "    )\n",
    "\n",
    "    # Combine PCA components with the target variable\n",
    "    pca_pd = pd.concat([pca_pd, pca_components], axis=1)\n",
    "\n",
    "    def scatter_plot(pca_pd, x, y):\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.scatterplot(x=x, y=y, hue=target, data=pca_pd, palette=\"viridis\", alpha=0.7)\n",
    "        plt.xlabel(x)\n",
    "        plt.ylabel(y)\n",
    "        plt.title(f\"PCA - {x} vs {y}\")\n",
    "        plt.legend(title=target)\n",
    "        plt.show()\n",
    "\n",
    "    # Plot foreach pair of principal components\n",
    "    for i in range(num_components):\n",
    "        for j in range(i + 1, num_components):\n",
    "            scatter_plot(pca_pd, pca_columns[i], pca_columns[j])\n",
    "\n",
    "\n",
    "# Plot PCA results\n",
    "plot_pca_scatter(pca_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significant_features_by_pca(pca_model, features = all_scaled_features):    \n",
    "    # Estrai il modello PCA\n",
    "    pca_stage = pca_model.stages[-1]\n",
    "    \n",
    "    # Ottieni le componenti principali\n",
    "    pca_components = pca_stage.pc.toArray()\n",
    "    \n",
    "    # Ottieni la varianza spiegata\n",
    "    explained_variance = pca_stage.explainedVariance.toArray()\n",
    "    \n",
    "    # Determina le features più significative\n",
    "    significant_features = {}\n",
    "    for i, ev in enumerate(explained_variance):\n",
    "        component = pca_components[:, i]\n",
    "        # Le features significative per questa componente\n",
    "        feature_contributions = {features[j]: abs(component[j]) for j in range(len(features))}\n",
    "        # Ordina per contributo in ordine decrescente\n",
    "        sorted_features = sorted(feature_contributions.items(), key=lambda x: x[1], reverse=True)\n",
    "        significant_features[f'PC{i+1}'] = sorted_features\n",
    "    \n",
    "    return significant_features, explained_variance\n",
    "\n",
    "# Esegui l'analisi PCA e determina le features più significative\n",
    "significant_features, explained_variance = significant_features_by_pca(pca_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExplained variance for each principal component:\")\n",
    "for i, ev in enumerate(explained_variance):\n",
    "    print(f\"PC{i+1}: {ev:.4f}\")\n",
    "\n",
    "print(\"\\nSum of explained variance:\", explained_variance.sum())\n",
    "\n",
    "print(\"\\nTop 10 features for each principal component:\")\n",
    "for pc, features in significant_features.items():\n",
    "    print(f\"\\n{pc}:\")\n",
    "    for feature, contribution in features[:10]:  # Mostra le prime 10 features per ogni componente\n",
    "        print(f\"{feature}: {contribution:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_scree(explained_variance):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, color='b', align='center')\n",
    "    plt.xlabel('Component Number')\n",
    "    plt.ylabel('Variance Explained')\n",
    "    plt.title('Scree Plot')\n",
    "    plt.show()\n",
    "\n",
    "def plot_cumulative_variance(explained_variance):\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Variance Explained')\n",
    "    plt.title('Cumulative Variance Explained by PCA')\n",
    "    plt.axhline(y=0.95, color='r', linestyle='--')\n",
    "    plt.text(0.5, 0.85, '95% Threshold', color='red', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "plot_scree(explained_variance)\n",
    "plot_cumulative_variance(explained_variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "def PCA_loadings(pca_model, features = FEATURES, k=COMPONENTS):\n",
    "    \n",
    "    # Extract PCA loadings\n",
    "    pca_stage = pca_model.stages[-1]\n",
    "    loadings = pca_stage.pc.toArray()\n",
    "    \n",
    "    # Create a DataFrame for loadings\n",
    "    loadings_df = pd.DataFrame(loadings, index=features, columns=[f'PC{i+1}' for i in range(k)])\n",
    "    \n",
    "    return loadings_df\n",
    "\n",
    "# Run PCA on the training data\n",
    "loadings_df = PCA_loadings(pca_model)\n",
    "\n",
    "# Display PCA loadings\n",
    "print(loadings_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph PCA loadings for each feature\n",
    "def plot_pca_loadings(loadings_df):\n",
    "    plt.figure(figsize=(24, 24))\n",
    "    sns.heatmap(loadings_df, annot=True, cmap='coolwarm')\n",
    "    plt.title('PCA Loadings')\n",
    "    plt.show()\n",
    "\n",
    "plot_pca_loadings(loadings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA loadings for the top 10 features\n",
    "top_features = loadings_df.abs().sum(axis=1).sort_values(ascending=False).head(10).index\n",
    "top_loadings_df = loadings_df.loc[top_features]\n",
    "plot_pca_loadings(top_loadings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Information (MI) Analysis for the features\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "pandas_df = encoded_train_data.toPandas()\n",
    "\n",
    "# Select features and target\n",
    "X = pandas_df[FEATURES]\n",
    "y = pandas_df[TARGET]\n",
    "\n",
    "# Calculate mutual information\n",
    "mi = mutual_info_classif(X, y)\n",
    "\n",
    "# Display Mutual Information scores\n",
    "feature_mi_scores = [(feature, score) for feature, score in zip(features, mi)]\n",
    "for feature, score in feature_mi_scores:\n",
    "    print(f\"Feature: {feature}, MI Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA revealed that the first two principal components explain approximately 69.17% of the variance in the dataset, with significant contributions from features such as FLOW_DURATION_MILLISECONDS, TCP_WIN_MAX_OUT, and L4_DST_PORT. The heatmap provided a visual representation of feature correlations, identifying relationships that guided feature selection. The MI analysis highlighted MAX_IP_PKT_LEN and LONGEST_FLOW_PKT as the most informative features for predicting the target variable, attack_index, indicating their potential importance in the classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scalling\n",
    "Feature scaling can play a crucial role in optimizing the performance of machine learning models, especially those sensitive to the scale of input data.\n",
    "Applying feature scaling to the right variables essential for improving model accuracy and efficiency. It ensures that each feature contributes equally to the decision-making process, preventing models from misinterpreting the data due to arbitrary feature scales. This leads to better, more reliable predictions in multiclass classification tasks.\n",
    "\n",
    "##### Numerical Variables with Different Scales:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "vscode_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
