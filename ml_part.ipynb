{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+--------------------------+--------------------+--------------------------+-----------------------+-----------------------+------------+\n",
      "| src_subnet_hashed|dst_subnet_vec|scaled_bytes_pkts_features|scaled_flow_duration|scaled_throughput_features|scaled_pkt_len_features|scaled_tcp_win_features|attack_index|\n",
      "+------------------+--------------+--------------------------+--------------------+--------------------------+-----------------------+-----------------------+------------+\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0513064471342...|[-0.4789529772200...|      [-0.0125662332574...|   [-0.4592399504915...|   [-0.4787111328395...|         1.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0513064471342...|[-0.4789529772200...|      [-0.0125662332574...|   [-0.4592399504915...|   [-0.4787111328395...|         1.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0513064471342...|[-0.4789529772200...|      [-0.0125662332574...|   [-0.4592399504915...|   [-0.4787111328395...|         1.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0275952189603...|  [2.08791390196378]|      [-0.0125633160103...|   [-0.4341342962160...|   [1.38149035044628...|         3.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0108895809287...|[-0.4789529772200...|      [-0.0125612606770...|   [0.00103037789422...|   [1.38149035044628...|         4.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0494742158662...|[-0.4789529772200...|      [-0.0125660078338...|   [-0.4236736069345...|   [-0.7822239306223...|         2.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0491508809365...|[-0.4789529772200...|      [-0.0125659680531...|   [-0.3504487819640...|   [-0.7822239306223...|         2.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0464564231895...|[-0.4789529772200...|      [-0.0125656365478...|   [-0.1893541670289...|   [-0.7822239306223...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0488275460069...|[-0.4789529772200...|      [-0.0125659282725...|   [-0.4111207797967...|   [-0.7822239306223...|         5.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0513064471342...|[-0.4789529772200...|      [-0.0125662332574...|   [-0.4592399504915...|   [-0.4787111328395...|         1.0|\n",
      "|(1024,[906],[1.0])|(94,[4],[1.0])|      [0.00743273175108...|[-0.4789529772200...|      [-0.0125590064406...|   [0.22907340423083...|   [1.38149035044628...|         6.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0039917690963...|[-0.4789529772200...|      [-0.0125604120233...|   [2.57854421685481...|   [1.38149035044628...|         4.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0513064471342...|[-0.4789529772200...|      [-0.0125662332574...|   [-0.4592399504915...|   [-0.4787111328395...|         1.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [-0.0517375603737...|[-0.4789529772200...|      [-0.0125662862983...|   [-0.4676085019167...|   [-0.7063457311766...|         0.0|\n",
      "|(1024,[906],[1.0])|(94,[0],[1.0])|      [0.37463343351662...|[-0.4789529772200...|      [-0.0125138288903...|   [-0.4759770533419...|   [-0.6606261520184...|         5.0|\n",
      "+------------------+--------------+--------------------------+--------------------+--------------------------+-----------------------+-----------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import parquet file with pySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session\n",
    "spark = SparkSession.builder.appName(\"parquet\").getOrCreate()\n",
    "\n",
    "# Read parquet file\n",
    "df = spark.read.parquet(\"ml_data_train.parquet\")\n",
    "\n",
    "# Show the data\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scalling\n",
    "Feature scaling can play a crucial role in optimizing the performance of machine learning models, especially those sensitive to the scale of input data.\n",
    "Applying feature scaling to the right variables essential for improving model accuracy and efficiency. It ensures that each feature contributes equally to the decision-making process, preventing models from misinterpreting the data due to arbitrary feature scales. This leads to better, more reliable predictions in multiclass classification tasks.\n",
    "\n",
    "##### Numerical Variables with Different Scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics summary for numerical variables candidates for scalling\n",
    "columns_of_interest_for_scaling = [\n",
    "    \"IN_BYTES\", \"OUT_BYTES\", \"IN_PKTS\", \"OUT_PKTS\", \n",
    "    \"FLOW_DURATION_MILLISECONDS\", \"SRC_TO_DST_SECOND_BYTES\", \n",
    "    \"DST_TO_SRC_SECOND_BYTES\", \"LONGEST_FLOW_PKT\", \"SHORTEST_FLOW_PKT\", \n",
    "    \"MIN_IP_PKT_LEN\", \"MAX_IP_PKT_LEN\", \"TCP_WIN_MAX_IN\", \"TCP_WIN_MAX_OUT\"\n",
    "]\n",
    "\n",
    "FEATURES = [\n",
    "    \"L4_SRC_PORT\",\n",
    "    \"L4_DST_PORT\",\n",
    "    #\"ICMP_TYPE\",\n",
    "    #\"ICMP_IPV4_TYPE\",\n",
    "    #\"DNS_QUERY_ID\",\n",
    "    #\"DNS_QUERY_TYPE\",\n",
    "    #\"DNS_TTL_ANSWER\",\n",
    "    #\"FTP_COMMAND_RET_CODE\",\n",
    "    #\"IN_BYTES\",\n",
    "    #\"OUT_BYTES\",\n",
    "    #\"IN_PKTS\",\n",
    "    #\"OUT_PKTS\",\n",
    "    #\"FLOW_DURATION_MILLISECONDS\",\n",
    "    #\"PROTOCOL\",\n",
    "    #\"SRC_TO_DST_SECOND_BYTES\",\n",
    "    #\"DST_TO_SRC_SECOND_BYTES\",\n",
    "    #\"LONGEST_FLOW_PKT\",\n",
    "    #\"SHORTEST_FLOW_PKT\",\n",
    "    #\"MIN_IP_PKT_LEN\",\n",
    "    #\"MAX_IP_PKT_LEN\",\n",
    "    #\"TCP_WIN_MAX_IN\",\n",
    "    #\"TCP_WIN_MAX_OUT\",\n",
    "    #\"dst_subnet_index\",\n",
    "    #\"src_subnet_index\",\n",
    "    #\"CLIENT_TCP_FLAGS\",\n",
    "    #\"SERVER_TCP_FLAGS\",\n",
    "    #\"DURATION_OUT\",\n",
    "    #\"DURATION_IN\",\n",
    "    #\"MIN_TTL\",\n",
    "    \"NUM_PKTS_UP_TO_128_BYTES\",\n",
    "    #\"RETRANSMITTED_IN_BYTES\",\n",
    "    #\"MAX_TTL\",\n",
    "    \"NUM_PKTS_128_TO_256_BYTES\",\n",
    "    #\"DST_TO_SRC_AVG_THROUGHPUT\",\n",
    "    \"NUM_PKTS_512_TO_1024_BYTES\",\n",
    "    \"NUM_PKTS_256_TO_512_BYTES\",\n",
    "    #\"RETRANSMITTED_OUT_BYTES\",\n",
    "    #\"RETRANSMITTED_OUT_PKTS\",\n",
    "    #\"RETRANSMITTED_IN_PKTS\",\n",
    "    #\"SRC_TO_DST_AVG_THROUGHPUT\",\n",
    "    #\"L7_PROTO\",\n",
    "    \"NUM_PKTS_1024_TO_1514_BYTES\",\n",
    "    #\"TCP_FLAGS\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, FeatureHasher, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# OneHot encoding for destination subnet since higher cardinality\n",
    "dst_subnet_encoder = OneHotEncoder(inputCol=\"dst_subnet_index\", outputCol=\"dst_subnet_vec\")\n",
    "\n",
    "# Source address\n",
    "# Feature hashing for source subnet since lower cardinality\n",
    "src_subnet_hasher = FeatureHasher(inputCols=[\"src_subnet_index\"], outputCol=\"src_subnet_hashed\", numFeatures=1024)\n",
    "\n",
    "# Assemble numerical features for bytes and packets trasmitted\n",
    "trasimitted_in_features = [\"IN_BYTES\", \"IN_PKTS\"]\n",
    "trasmitted_out_features = [\"OUT_BYTES\", \"OUT_PKTS\"]\n",
    "retransimtted_in_features = [\"RETRANSMITTED_IN_BYTES\", \"RETRANSMITTED_IN_PKTS\"]\n",
    "retransimtted_out_features = [\"RETRANSMITTED_OUT_BYTES\", \"RETRANSMITTED_OUT_PKTS\"]\n",
    "throughput_in_features = [\"SRC_TO_DST_SECOND_BYTES\", \"SRC_TO_DST_AVG_THROUGHPUT\"]\n",
    "throughput_out_features = [\"DST_TO_SRC_AVG_THROUGHPUT\", \"DST_TO_SRC_SECOND_BYTES\"]\n",
    "ttl_features = [\"MIN_TTL\", \"MAX_TTL\"]\n",
    "tcp_flags_features = [\"CLIENT_TCP_FLAGS\", \"SERVER_TCP_FLAGS\", \"TCP_FLAGS\"]\n",
    "icmp_features = [\"ICMP_TYPE\", \"ICMP_IPV4_TYPE\"]\n",
    "flow_max_features = [\"LONGEST_FLOW_PKT\", \"MAX_IP_PKT_LEN\"]\n",
    "flow_min_features = [\"SHORTEST_FLOW_PKT\", \"MIN_IP_PKT_LEN\"]\n",
    "flow_duration_features = [\"FLOW_DURATION_MILLISECONDS\"]\n",
    "tcp_in_features = [\"TCP_WIN_MAX_IN\"]\n",
    "tcp_out_features = [\"TCP_WIN_MAX_OUT\"]\n",
    "duration_in_features = [\"DURATION_IN\"]\n",
    "duration_out_features = [\"DURATION_OUT\"]\n",
    "protocol_features = [\"PROTOCOL\"]\n",
    "l4_src_port_features = [\"L4_SRC_PORT\"]\n",
    "l4_dst_port_features = [\"L4_DST_PORT\"]\n",
    "dns_features = [\"DNS_QUERY_ID\", \"DNS_QUERY_TYPE\", \"DNS_TTL_ANSWER\"]\n",
    "ftp_features = [\"FTP_COMMAND_RET_CODE\"]\n",
    "l7_proto_features = [\"L7_PROTO\"]\n",
    "num_packets_by_size_features = [\"NUM_PKTS_UP_TO_128_BYTES\", \"NUM_PKTS_128_TO_256_BYTES\", \"NUM_PKTS_256_TO_512_BYTES\", \"NUM_PKTS_512_TO_1024_BYTES\", \"NUM_PKTS_1024_TO_1514_BYTES\"]\n",
    "\n",
    "\n",
    "\n",
    "# Assemble numerical features for flow duration\n",
    "flow_duration_assembler = VectorAssembler(\n",
    "    inputCols=[\"FLOW_DURATION_MILLISECONDS\"],\n",
    "    outputCol=\"flow_duration_feature\"\n",
    ")\n",
    "\n",
    "tcp__assembler = VectorAssembler(\n",
    "    inputCols=[\"TCP_WIN_MAX_IN\", \"TCP_WIN_MAX_OUT\"],\n",
    "    outputCol=\"tcp_win_features\",\n",
    ")\n",
    "\n",
    "# Apply StandardScaler to the assembled vectors\n",
    "bytes_pkts_scaler = StandardScaler(\n",
    "    inputCol=\"bytes_pkts_features\", \n",
    "    outputCol=\"scaled_bytes_pkts_features\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "flow_duration_scaler = StandardScaler(\n",
    "    inputCol=\"flow_duration_feature\", \n",
    "    outputCol=\"scaled_flow_duration\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "throughput_scaler = StandardScaler(\n",
    "    inputCol=\"throughput_features\", \n",
    "    outputCol=\"scaled_throughput_features\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "pkt_len_scaler = StandardScaler(\n",
    "    inputCol=\"pkt_len_features\", \n",
    "    outputCol=\"scaled_pkt_len_features\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "tcp_win_scaler = StandardScaler(\n",
    "    inputCol=\"tcp_win_features\", \n",
    "    outputCol=\"scaled_tcp_win_features\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Create a pipeline for scaling\n",
    "scaling_pipeline = Pipeline(stages=[\n",
    "    dst_subnet_encoder,\n",
    "    src_subnet_hasher,\n",
    "    bytes_pkts_assembler, \n",
    "    bytes_pkts_scaler, \n",
    "    flow_duration_assembler, \n",
    "    flow_duration_scaler, \n",
    "    throughput_assembler, \n",
    "    throughput_scaler, \n",
    "    pkt_len_assembler, \n",
    "    pkt_len_scaler, \n",
    "    tcp_win_assembler, \n",
    "    tcp_win_scaler\n",
    "])\n",
    "\n",
    "# Fit the scalling pipeline to the encoded training data\n",
    "scalling_model = scaling_pipeline.fit(encoded_train_data)\n",
    "\n",
    "# Transform both encoded training and test data\n",
    "processed_train_data = scalling_model.transform(encoded_train_data)\n",
    "processed_test_data = scalling_model.transform(encoded_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Preparar os dados com VectorAssembler\n",
    "feature_columns = ['scaled_bytes_pkts_features', 'scaled_flow_duration', 'scaled_throughput_features', 'scaled_pkt_len_features', 'scaled_tcp_win_features']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, NaiveBayes, LogisticRegression, MultilayerPerceptronClassifier\n",
    "\n",
    "# Definir o Random Forest\n",
    "rf = RandomForestClassifier(labelCol=\"attack_index\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# Definir o Naive Bayes\n",
    "#nb = NaiveBayes(labelCol=\"attack_index\", featuresCol=\"features\")\n",
    "\n",
    "# Definir o Logistic Regression\n",
    "lr = LogisticRegression(labelCol=\"attack_index\", featuresCol=\"features\")\n",
    "\n",
    "# Configuração do Multilayer Perceptron\n",
    "# Obter o número de características\n",
    "def get_feature_count(df, feature_col=\"features\"):\n",
    "    # Extrai os metadados da coluna de características e calcula a soma dos tamanhos dos atributos\n",
    "    attributes = df.schema[feature_col].metadata[\"ml_attr\"][\"attrs\"]\n",
    "    feature_count = sum(len(attrs) for attrs in attributes.values())\n",
    "    return feature_count\n",
    "input_layers = get_feature_count(df_assembled, \"features\")\n",
    "output_layers = df_assembled.select(\"attack_index\").distinct().count()\n",
    "hidden_layers = [input_layers, (input_layers + output_layers) // 2, output_layers]\n",
    "mlp = MultilayerPerceptronClassifier(labelCol=\"attack_index\", featuresCol=\"features\", layers=hidden_layers, maxIter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento dos modelos\n",
    "model_rf = rf.fit(df_assembled)\n",
    "#model_nb = nb.fit(df_assembled)\n",
    "model_lr = lr.fit(df_assembled)\n",
    "model_mlp = mlp.fit(df_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8129\n",
      "Logistic Regression Accuracy: 0.6209\n",
      "Multilayer Perceptron Accuracy: 0.8125\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Avaliador\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"attack_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Avaliar Random Forest\n",
    "accuracy_rf = evaluator.evaluate(model_rf.transform(df_assembled))\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf}\")\n",
    "\n",
    "# Avaliar Naive Bayes\n",
    "#accuracy_nb = evaluator.evaluate(model_nb.transform(df_assembled))\n",
    "#print(f\"Naive Bayes Accuracy: {accuracy_nb}\")\n",
    "\n",
    "# Avaliar Logistic Regression\n",
    "accuracy_lr = evaluator.evaluate(model_lr.transform(df_assembled))\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_lr}\")\n",
    "\n",
    "# Avaliar Multilayer Perceptron\n",
    "accuracy_mlp = evaluator.evaluate(model_mlp.transform(df_assembled))\n",
    "print(f\"Multilayer Perceptron Accuracy: {accuracy_mlp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "vscode_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
