{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REMOVE CELL\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "# Group similar features\n",
    "trasimitted_in_features = [\"IN_BYTES\", \"IN_PKTS\"]\n",
    "trasmitted_out_features = [\"OUT_BYTES\", \"OUT_PKTS\"]\n",
    "retransimtted_in_features = [\"RETRANSMITTED_IN_BYTES\", \"RETRANSMITTED_IN_PKTS\"]\n",
    "retransimtted_out_features = [\"RETRANSMITTED_OUT_BYTES\", \"RETRANSMITTED_OUT_PKTS\"]\n",
    "throughput_in_features = [\"SRC_TO_DST_SECOND_BYTES\", \"SRC_TO_DST_AVG_THROUGHPUT\"]\n",
    "throughput_out_features = [\"DST_TO_SRC_AVG_THROUGHPUT\", \"DST_TO_SRC_SECOND_BYTES\"]\n",
    "ttl_features = [\"MIN_TTL\", \"MAX_TTL\"]\n",
    "tcp_flags_features = [\"CLIENT_TCP_FLAGS\", \"SERVER_TCP_FLAGS\", \"TCP_FLAGS\"]\n",
    "icmp_features = [\"ICMP_TYPE\", \"ICMP_IPV4_TYPE\"]\n",
    "flow_max_features = [\"LONGEST_FLOW_PKT\", \"MAX_IP_PKT_LEN\"]\n",
    "flow_min_features = [\"SHORTEST_FLOW_PKT\", \"MIN_IP_PKT_LEN\"]\n",
    "flow_duration_features = [\"FLOW_DURATION_MILLISECONDS\"]\n",
    "tcp_in_features = [\"TCP_WIN_MAX_IN\"]\n",
    "tcp_out_features = [\"TCP_WIN_MAX_OUT\"]\n",
    "\n",
    "# Assemble and scale each group\n",
    "assemblers_and_scalers = []\n",
    "\n",
    "def create_assembler_and_scaler(inputCols, outputCol):\n",
    "    assembler = VectorAssembler(inputCols=inputCols, outputCol=outputCol)\n",
    "    scaler = StandardScaler(inputCol=outputCol, outputCol=f\"scaled_{outputCol}\", withStd=True, withMean=True)\n",
    "    return assembler, scaler\n",
    "\n",
    "grouped_features = {\n",
    "    \"trasmitted_in\": trasimitted_in_features,\n",
    "    \"trasmitted_out\": trasmitted_out_features,\n",
    "    \"retransimtted_in\": retransimtted_in_features,\n",
    "    \"retransimtted_out\": retransimtted_out_features,\n",
    "    \"throughput_in\": throughput_in_features,\n",
    "    \"throughput_out\": throughput_out_features,\n",
    "    \"ttl\": ttl_features,\n",
    "    \"tcp_flags\": tcp_flags_features,\n",
    "    \"icmp\": icmp_features,\n",
    "    \"flow_max\": flow_max_features,\n",
    "    \"flow_min\": flow_min_features,\n",
    "    \"flow_duration\": flow_duration_features,\n",
    "    \"tcp_in\": tcp_in_features,\n",
    "    \"tcp_out\": tcp_out_features,\n",
    "}\n",
    "\n",
    "for group_name, features in grouped_features.items():\n",
    "    assembler, scaler = create_assembler_and_scaler(features, f\"{group_name}_features\")\n",
    "    assemblers_and_scalers.append(assembler)\n",
    "    assemblers_and_scalers.append(scaler)\n",
    "\n",
    "# Assemble all scaled features into a single feature vector\n",
    "all_scaled_features = [f\"scaled_{group_name}_features\" for group_name in grouped_features.keys()]\n",
    "final_assembler = VectorAssembler(inputCols=all_scaled_features, outputCol=\"final_features\")\n",
    "\n",
    "# Add PCA\n",
    "COMPONENTS = 3  # Number of desired principal components\n",
    "pca = PCA(k=COMPONENTS, inputCol=\"final_features\", outputCol=\"pca_features\")\n",
    "\n",
    "# Create the complete pipeline\n",
    "pipeline = Pipeline(stages=assemblers_and_scalers + [final_assembler, pca])\n",
    "\n",
    "# Fit the pipeline and transform the data\n",
    "pca_model = pipeline.fit(encoded_train_data)\n",
    "pca_result = pca_model.transform(encoded_train_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "scaled_trasmitted_in_features does not exist. Available: L4_SRC_PORT, L4_DST_PORT, PROTOCOL, L7_PROTO, IN_BYTES, IN_PKTS, OUT_BYTES, OUT_PKTS, TCP_FLAGS, CLIENT_TCP_FLAGS, SERVER_TCP_FLAGS, FLOW_DURATION_MILLISECONDS, DURATION_IN, DURATION_OUT, MIN_TTL, MAX_TTL, LONGEST_FLOW_PKT, SHORTEST_FLOW_PKT, MIN_IP_PKT_LEN, MAX_IP_PKT_LEN, SRC_TO_DST_SECOND_BYTES, DST_TO_SRC_SECOND_BYTES, RETRANSMITTED_IN_BYTES, RETRANSMITTED_IN_PKTS, RETRANSMITTED_OUT_BYTES, RETRANSMITTED_OUT_PKTS, SRC_TO_DST_AVG_THROUGHPUT, DST_TO_SRC_AVG_THROUGHPUT, NUM_PKTS_UP_TO_128_BYTES, NUM_PKTS_128_TO_256_BYTES, NUM_PKTS_256_TO_512_BYTES, NUM_PKTS_512_TO_1024_BYTES, NUM_PKTS_1024_TO_1514_BYTES, TCP_WIN_MAX_IN, TCP_WIN_MAX_OUT, ICMP_TYPE, ICMP_IPV4_TYPE, DNS_QUERY_ID, DNS_QUERY_TYPE, DNS_TTL_ANSWER, FTP_COMMAND_RET_CODE, attack_index, dst_subnet_index, src_subnet_index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[27], line 20\u001b[0m\n",
      "\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pca_result, pca_model\n",
      "\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Perform PCA analysis\u001b[39;00m\n",
      "\u001b[0;32m---> 20\u001b[0m pca_result, pca_model \u001b[38;5;241m=\u001b[39m \u001b[43mPCA_analysis_variance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_train_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[0;32mIn[27], line 13\u001b[0m, in \u001b[0;36mPCA_analysis_variance\u001b[0;34m(data, features, components)\u001b[0m\n",
      "\u001b[1;32m     11\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(k\u001b[38;5;241m=\u001b[39mcomponents, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaled_features\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m     12\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[assembler, scaler, pca])\n",
      "\u001b[0;32m---> 13\u001b[0m pca_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     14\u001b[0m pca_result \u001b[38;5;241m=\u001b[39m pca_model\u001b[38;5;241m.\u001b[39mtransform(data)\n",
      "\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pca_result, pca_model\n",
      "\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n",
      "\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n",
      "\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n",
      "\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n",
      "\u001b[1;32m    210\u001b[0m     )\n",
      "\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/ml/pipeline.py:132\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n",
      "\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stage, Transformer):\n",
      "\u001b[1;32m    131\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(stage)\n",
      "\u001b[0;32m--> 132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n",
      "\u001b[1;32m    134\u001b[0m     model \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mfit(dataset)\n",
      "\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n",
      "\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n",
      "\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/ml/wrapper.py:398\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n",
      "\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n",
      "\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
      "\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
      "\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
      "\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
      "\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
      "\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
      "\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
      "\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
      "\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
      "\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: scaled_trasmitted_in_features does not exist. Available: L4_SRC_PORT, L4_DST_PORT, PROTOCOL, L7_PROTO, IN_BYTES, IN_PKTS, OUT_BYTES, OUT_PKTS, TCP_FLAGS, CLIENT_TCP_FLAGS, SERVER_TCP_FLAGS, FLOW_DURATION_MILLISECONDS, DURATION_IN, DURATION_OUT, MIN_TTL, MAX_TTL, LONGEST_FLOW_PKT, SHORTEST_FLOW_PKT, MIN_IP_PKT_LEN, MAX_IP_PKT_LEN, SRC_TO_DST_SECOND_BYTES, DST_TO_SRC_SECOND_BYTES, RETRANSMITTED_IN_BYTES, RETRANSMITTED_IN_PKTS, RETRANSMITTED_OUT_BYTES, RETRANSMITTED_OUT_PKTS, SRC_TO_DST_AVG_THROUGHPUT, DST_TO_SRC_AVG_THROUGHPUT, NUM_PKTS_UP_TO_128_BYTES, NUM_PKTS_128_TO_256_BYTES, NUM_PKTS_256_TO_512_BYTES, NUM_PKTS_512_TO_1024_BYTES, NUM_PKTS_1024_TO_1514_BYTES, TCP_WIN_MAX_IN, TCP_WIN_MAX_OUT, ICMP_TYPE, ICMP_IPV4_TYPE, DNS_QUERY_ID, DNS_QUERY_TYPE, DNS_TTL_ANSWER, FTP_COMMAND_RET_CODE, attack_index, dst_subnet_index, src_subnet_index"
     ]
    }
   ],
   "source": [
    "## REMOVE CELL\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "## COPY THIS AND TRY WITH ML DATASET\n",
    "FEATURES = all_scaled_features\n",
    "COMPONENTS = 3\n",
    "def PCA_analysis_variance(data, features=FEATURES, components=COMPONENTS):\n",
    "    # Esegui l'analisi PCA e ottieni i risultati\n",
    "    assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True\n",
    "    )\n",
    "    pca = PCA(k=components, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, pca])\n",
    "    pca_model = pipeline.fit(data)\n",
    "    pca_result = pca_model.transform(data)\n",
    "\n",
    "    return pca_result, pca_model\n",
    "\n",
    "\n",
    "# Perform PCA analysis\n",
    "pca_result, pca_model = PCA_analysis_variance(encoded_train_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
