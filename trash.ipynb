{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REMOVE CELL\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "# Group similar features\n",
    "trasimitted_in_features = [\"IN_BYTES\", \"IN_PKTS\"]\n",
    "trasmitted_out_features = [\"OUT_BYTES\", \"OUT_PKTS\"]\n",
    "retransimtted_in_features = [\"RETRANSMITTED_IN_BYTES\", \"RETRANSMITTED_IN_PKTS\"]\n",
    "retransimtted_out_features = [\"RETRANSMITTED_OUT_BYTES\", \"RETRANSMITTED_OUT_PKTS\"]\n",
    "throughput_in_features = [\"SRC_TO_DST_SECOND_BYTES\", \"SRC_TO_DST_AVG_THROUGHPUT\"]\n",
    "throughput_out_features = [\"DST_TO_SRC_AVG_THROUGHPUT\", \"DST_TO_SRC_SECOND_BYTES\"]\n",
    "ttl_features = [\"MIN_TTL\", \"MAX_TTL\"]\n",
    "tcp_flags_features = [\"CLIENT_TCP_FLAGS\", \"SERVER_TCP_FLAGS\", \"TCP_FLAGS\"]\n",
    "icmp_features = [\"ICMP_TYPE\", \"ICMP_IPV4_TYPE\"]\n",
    "flow_max_features = [\"LONGEST_FLOW_PKT\", \"MAX_IP_PKT_LEN\"]\n",
    "flow_min_features = [\"SHORTEST_FLOW_PKT\", \"MIN_IP_PKT_LEN\"]\n",
    "flow_duration_features = [\"FLOW_DURATION_MILLISECONDS\"]\n",
    "tcp_in_features = [\"TCP_WIN_MAX_IN\"]\n",
    "tcp_out_features = [\"TCP_WIN_MAX_OUT\"]\n",
    "\n",
    "# Assemble and scale each group\n",
    "assemblers_and_scalers = []\n",
    "\n",
    "def create_assembler_and_scaler(inputCols, outputCol):\n",
    "    assembler = VectorAssembler(inputCols=inputCols, outputCol=outputCol)\n",
    "    scaler = StandardScaler(inputCol=outputCol, outputCol=f\"scaled_{outputCol}\", withStd=True, withMean=True)\n",
    "    return assembler, scaler\n",
    "\n",
    "grouped_features = {\n",
    "    \"trasmitted_in\": trasimitted_in_features,\n",
    "    \"trasmitted_out\": trasmitted_out_features,\n",
    "    \"retransimtted_in\": retransimtted_in_features,\n",
    "    \"retransimtted_out\": retransimtted_out_features,\n",
    "    \"throughput_in\": throughput_in_features,\n",
    "    \"throughput_out\": throughput_out_features,\n",
    "    \"ttl\": ttl_features,\n",
    "    \"tcp_flags\": tcp_flags_features,\n",
    "    \"icmp\": icmp_features,\n",
    "    \"flow_max\": flow_max_features,\n",
    "    \"flow_min\": flow_min_features,\n",
    "    \"flow_duration\": flow_duration_features,\n",
    "    \"tcp_in\": tcp_in_features,\n",
    "    \"tcp_out\": tcp_out_features,\n",
    "}\n",
    "\n",
    "for group_name, features in grouped_features.items():\n",
    "    assembler, scaler = create_assembler_and_scaler(features, f\"{group_name}_features\")\n",
    "    assemblers_and_scalers.append(assembler)\n",
    "    assemblers_and_scalers.append(scaler)\n",
    "\n",
    "# Assemble all scaled features into a single feature vector\n",
    "all_scaled_features = [f\"scaled_{group_name}_features\" for group_name in grouped_features.keys()]\n",
    "final_assembler = VectorAssembler(inputCols=all_scaled_features, outputCol=\"final_features\")\n",
    "\n",
    "# Add PCA\n",
    "COMPONENTS = 3  # Number of desired principal components\n",
    "pca = PCA(k=COMPONENTS, inputCol=\"final_features\", outputCol=\"pca_features\")\n",
    "\n",
    "# Create the complete pipeline\n",
    "pipeline = Pipeline(stages=assemblers_and_scalers + [final_assembler, pca])\n",
    "\n",
    "# Fit the pipeline and transform the data\n",
    "pca_model = pipeline.fit(encoded_train_data)\n",
    "pca_result = pca_model.transform(encoded_train_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "scaled_trasmitted_in_features does not exist. Available: L4_SRC_PORT, L4_DST_PORT, PROTOCOL, L7_PROTO, IN_BYTES, IN_PKTS, OUT_BYTES, OUT_PKTS, TCP_FLAGS, CLIENT_TCP_FLAGS, SERVER_TCP_FLAGS, FLOW_DURATION_MILLISECONDS, DURATION_IN, DURATION_OUT, MIN_TTL, MAX_TTL, LONGEST_FLOW_PKT, SHORTEST_FLOW_PKT, MIN_IP_PKT_LEN, MAX_IP_PKT_LEN, SRC_TO_DST_SECOND_BYTES, DST_TO_SRC_SECOND_BYTES, RETRANSMITTED_IN_BYTES, RETRANSMITTED_IN_PKTS, RETRANSMITTED_OUT_BYTES, RETRANSMITTED_OUT_PKTS, SRC_TO_DST_AVG_THROUGHPUT, DST_TO_SRC_AVG_THROUGHPUT, NUM_PKTS_UP_TO_128_BYTES, NUM_PKTS_128_TO_256_BYTES, NUM_PKTS_256_TO_512_BYTES, NUM_PKTS_512_TO_1024_BYTES, NUM_PKTS_1024_TO_1514_BYTES, TCP_WIN_MAX_IN, TCP_WIN_MAX_OUT, ICMP_TYPE, ICMP_IPV4_TYPE, DNS_QUERY_ID, DNS_QUERY_TYPE, DNS_TTL_ANSWER, FTP_COMMAND_RET_CODE, attack_index, dst_subnet_index, src_subnet_index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[27], line 20\u001b[0m\n",
      "\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pca_result, pca_model\n",
      "\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Perform PCA analysis\u001b[39;00m\n",
      "\u001b[0;32m---> 20\u001b[0m pca_result, pca_model \u001b[38;5;241m=\u001b[39m \u001b[43mPCA_analysis_variance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_train_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[0;32mIn[27], line 13\u001b[0m, in \u001b[0;36mPCA_analysis_variance\u001b[0;34m(data, features, components)\u001b[0m\n",
      "\u001b[1;32m     11\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(k\u001b[38;5;241m=\u001b[39mcomponents, inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaled_features\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m     12\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[assembler, scaler, pca])\n",
      "\u001b[0;32m---> 13\u001b[0m pca_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     14\u001b[0m pca_result \u001b[38;5;241m=\u001b[39m pca_model\u001b[38;5;241m.\u001b[39mtransform(data)\n",
      "\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pca_result, pca_model\n",
      "\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n",
      "\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n",
      "\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n",
      "\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n",
      "\u001b[1;32m    210\u001b[0m     )\n",
      "\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/ml/pipeline.py:132\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n",
      "\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stage, Transformer):\n",
      "\u001b[1;32m    131\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(stage)\n",
      "\u001b[0;32m--> 132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n",
      "\u001b[1;32m    134\u001b[0m     model \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mfit(dataset)\n",
      "\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n",
      "\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n",
      "\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/ml/wrapper.py:398\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n",
      "\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n",
      "\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
      "\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
      "\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
      "\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
      "\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
      "\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
      "\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
      "\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
      "\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
      "\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: scaled_trasmitted_in_features does not exist. Available: L4_SRC_PORT, L4_DST_PORT, PROTOCOL, L7_PROTO, IN_BYTES, IN_PKTS, OUT_BYTES, OUT_PKTS, TCP_FLAGS, CLIENT_TCP_FLAGS, SERVER_TCP_FLAGS, FLOW_DURATION_MILLISECONDS, DURATION_IN, DURATION_OUT, MIN_TTL, MAX_TTL, LONGEST_FLOW_PKT, SHORTEST_FLOW_PKT, MIN_IP_PKT_LEN, MAX_IP_PKT_LEN, SRC_TO_DST_SECOND_BYTES, DST_TO_SRC_SECOND_BYTES, RETRANSMITTED_IN_BYTES, RETRANSMITTED_IN_PKTS, RETRANSMITTED_OUT_BYTES, RETRANSMITTED_OUT_PKTS, SRC_TO_DST_AVG_THROUGHPUT, DST_TO_SRC_AVG_THROUGHPUT, NUM_PKTS_UP_TO_128_BYTES, NUM_PKTS_128_TO_256_BYTES, NUM_PKTS_256_TO_512_BYTES, NUM_PKTS_512_TO_1024_BYTES, NUM_PKTS_1024_TO_1514_BYTES, TCP_WIN_MAX_IN, TCP_WIN_MAX_OUT, ICMP_TYPE, ICMP_IPV4_TYPE, DNS_QUERY_ID, DNS_QUERY_TYPE, DNS_TTL_ANSWER, FTP_COMMAND_RET_CODE, attack_index, dst_subnet_index, src_subnet_index"
     ]
    }
   ],
   "source": [
    "## REMOVE CELL\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "## COPY THIS AND TRY WITH ML DATASET\n",
    "FEATURES = all_scaled_features\n",
    "COMPONENTS = 3\n",
    "def PCA_analysis_variance(data, features=FEATURES, components=COMPONENTS):\n",
    "    # Esegui l'analisi PCA e ottieni i risultati\n",
    "    assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True\n",
    "    )\n",
    "    pca = PCA(k=components, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, pca])\n",
    "    pca_model = pipeline.fit(data)\n",
    "    pca_result = pca_model.transform(data)\n",
    "\n",
    "    return pca_result, pca_model\n",
    "\n",
    "\n",
    "# Perform PCA analysis\n",
    "pca_result, pca_model = PCA_analysis_variance(encoded_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Multiclass classification IoT\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "def load_test_data(debug=False):\n",
    "    if debug:\n",
    "        train_data = spark.read.parquet(\"ml_data_train.parquet\").limit(300000)\n",
    "        \n",
    "    else:\n",
    "        train_data = spark.read.parquet(\"simplified_train_data.parquet\")\n",
    "        \n",
    "    return train_data\n",
    "\n",
    "processed_train_data = load_test_data(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (14, 3), indices imply (8, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[71], line 17\u001b[0m\n",
      "\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loadings_df\n",
      "\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Run PCA on the training data\u001b[39;00m\n",
      "\u001b[0;32m---> 17\u001b[0m loadings_df \u001b[38;5;241m=\u001b[39m \u001b[43mPCA_loadings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpca_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Display PCA loadings\u001b[39;00m\n",
      "\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(loadings_df)\n",
      "\n",
      "Cell \u001b[0;32mIn[71], line 12\u001b[0m, in \u001b[0;36mPCA_loadings\u001b[0;34m(pca_model, features, k)\u001b[0m\n",
      "\u001b[1;32m      9\u001b[0m loadings \u001b[38;5;241m=\u001b[39m pca_stage\u001b[38;5;241m.\u001b[39mpc\u001b[38;5;241m.\u001b[39mtoArray()\n",
      "\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create a DataFrame for loadings\u001b[39;00m\n",
      "\u001b[0;32m---> 12\u001b[0m loadings_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloadings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPC\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loadings_df\n",
      "\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pandas/core/frame.py:827\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n",
      "\u001b[1;32m    816\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n",
      "\u001b[1;32m    817\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n",
      "\u001b[1;32m    818\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    824\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n",
      "\u001b[1;32m    825\u001b[0m         )\n",
      "\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 827\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    828\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    829\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    830\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    831\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    833\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    836\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n",
      "\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pandas/core/internals/construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n",
      "\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n",
      "\u001b[1;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n",
      "\u001b[1;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n",
      "\u001b[1;32m    334\u001b[0m )\n",
      "\u001b[0;32m--> 336\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pandas/core/internals/construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n",
      "\u001b[1;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n",
      "\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (14, 3), indices imply (8, 3)"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "def PCA_loadings(pca_model, features = FEATURES, k=COMPONENTS):\n",
    "    \n",
    "    # Extract PCA loadings\n",
    "    pca_stage = pca_model.stages[-1]\n",
    "    loadings = pca_stage.pc.toArray()\n",
    "    \n",
    "    # Create a DataFrame for loadings\n",
    "    loadings_df = pd.DataFrame(loadings, index=features, columns=[f'PC{i+1}' for i in range(k)])\n",
    "    \n",
    "    return loadings_df\n",
    "\n",
    "# Run PCA on the training data\n",
    "loadings_df = PCA_loadings(pca_model)\n",
    "\n",
    "# Display PCA loadings\n",
    "print(loadings_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph PCA loadings for each feature\n",
    "def plot_pca_loadings(loadings_df):\n",
    "    plt.figure(figsize=(24, 24))\n",
    "    sns.heatmap(loadings_df, annot=True, cmap='coolwarm')\n",
    "    plt.title('PCA Loadings')\n",
    "    plt.show()\n",
    "\n",
    "plot_pca_loadings(loadings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA loadings for the top 10 features\n",
    "top_features = loadings_df.abs().sum(axis=1).sort_values(ascending=False).head(10).index\n",
    "top_loadings_df = loadings_df.loc[top_features]\n",
    "plot_pca_loadings(top_loadings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, FeatureHasher, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# OneHot encoding for destination subnet since higher cardinality\n",
    "dst_subnet_encoder = OneHotEncoder(inputCol=\"dst_subnet_index\", outputCol=\"dst_subnet_vec\")\n",
    "\n",
    "# Source address\n",
    "# Feature hashing for source subnet since lower cardinality\n",
    "src_subnet_hasher = FeatureHasher(inputCols=[\"src_subnet_index\"], outputCol=\"src_subnet_hashed\", numFeatures=1024)\n",
    "\n",
    "# Assemble numerical features for bytes and packets trasmitted\n",
    "trasimitted_in_features = [\"IN_BYTES\", \"IN_PKTS\"]\n",
    "trasmitted_out_features = [\"OUT_BYTES\", \"OUT_PKTS\"]\n",
    "retransimtted_in_features = [\"RETRANSMITTED_IN_BYTES\", \"RETRANSMITTED_IN_PKTS\"]\n",
    "retransimtted_out_features = [\"RETRANSMITTED_OUT_BYTES\", \"RETRANSMITTED_OUT_PKTS\"]\n",
    "throughput_in_features = [\"SRC_TO_DST_SECOND_BYTES\", \"SRC_TO_DST_AVG_THROUGHPUT\"]\n",
    "throughput_out_features = [\"DST_TO_SRC_AVG_THROUGHPUT\", \"DST_TO_SRC_SECOND_BYTES\"]\n",
    "ttl_features = [\"MIN_TTL\", \"MAX_TTL\"]\n",
    "tcp_flags_features = [\"CLIENT_TCP_FLAGS\", \"SERVER_TCP_FLAGS\", \"TCP_FLAGS\"]\n",
    "icmp_features = [\"ICMP_TYPE\", \"ICMP_IPV4_TYPE\"]\n",
    "flow_max_features = [\"LONGEST_FLOW_PKT\", \"MAX_IP_PKT_LEN\"]\n",
    "flow_min_features = [\"SHORTEST_FLOW_PKT\", \"MIN_IP_PKT_LEN\"]\n",
    "flow_duration_features = [\"FLOW_DURATION_MILLISECONDS\"]\n",
    "tcp_in_features = [\"TCP_WIN_MAX_IN\"]\n",
    "tcp_out_features = [\"TCP_WIN_MAX_OUT\"]\n",
    "duration_in_features = [\"DURATION_IN\"]\n",
    "duration_out_features = [\"DURATION_OUT\"]\n",
    "protocol_features = [\"PROTOCOL\"]\n",
    "l4_src_port_features = [\"L4_SRC_PORT\"]\n",
    "l4_dst_port_features = [\"L4_DST_PORT\"]\n",
    "dns_features = [\"DNS_QUERY_ID\", \"DNS_QUERY_TYPE\", \"DNS_TTL_ANSWER\"]\n",
    "ftp_features = [\"FTP_COMMAND_RET_CODE\"]\n",
    "l7_proto_features = [\"L7_PROTO\"]\n",
    "num_packets_by_size_features = [\"NUM_PKTS_UP_TO_128_BYTES\", \"NUM_PKTS_128_TO_256_BYTES\", \"NUM_PKTS_256_TO_512_BYTES\", \"NUM_PKTS_512_TO_1024_BYTES\", \"NUM_PKTS_1024_TO_1514_BYTES\"]\n",
    "\n",
    "\n",
    "\n",
    "# Assemble numerical features for flow duration\n",
    "flow_duration_assembler = VectorAssembler(\n",
    "    inputCols=[\"FLOW_DURATION_MILLISECONDS\"],\n",
    "    outputCol=\"flow_duration_feature\"\n",
    ")\n",
    "\n",
    "tcp__assembler = VectorAssembler(\n",
    "    inputCols=[\"TCP_WIN_MAX_IN\", \"TCP_WIN_MAX_OUT\"],\n",
    "    outputCol=\"tcp_win_features\",\n",
    ")\n",
    "\n",
    "# Apply StandardScaler to the assembled vectors\n",
    "bytes_pkts_scaler = StandardScaler(\n",
    "    inputCol=\"bytes_pkts_features\", \n",
    "    outputCol=\"scaled_bytes_pkts_features\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "flow_duration_scaler = StandardScaler(\n",
    "    inputCol=\"flow_duration_feature\", \n",
    "    outputCol=\"scaled_flow_duration\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "throughput_scaler = StandardScaler(\n",
    "    inputCol=\"throughput_features\", \n",
    "    outputCol=\"scaled_throughput_features\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "pkt_len_scaler = StandardScaler(\n",
    "    inputCol=\"pkt_len_features\", \n",
    "    outputCol=\"scaled_pkt_len_features\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "tcp_win_scaler = StandardScaler(\n",
    "    inputCol=\"tcp_win_features\", \n",
    "    outputCol=\"scaled_tcp_win_features\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Create a pipeline for scaling\n",
    "scaling_pipeline = Pipeline(stages=[\n",
    "    dst_subnet_encoder,\n",
    "    src_subnet_hasher,\n",
    "    bytes_pkts_assembler, \n",
    "    bytes_pkts_scaler, \n",
    "    flow_duration_assembler, \n",
    "    flow_duration_scaler, \n",
    "    throughput_assembler, \n",
    "    throughput_scaler, \n",
    "    pkt_len_assembler, \n",
    "    pkt_len_scaler, \n",
    "    tcp_win_assembler, \n",
    "    tcp_win_scaler\n",
    "])\n",
    "\n",
    "# Fit the scalling pipeline to the encoded training data\n",
    "scalling_model = scaling_pipeline.fit(encoded_train_data)\n",
    "\n",
    "# Transform both encoded training and test data\n",
    "processed_train_data = scalling_model.transform(encoded_train_data)\n",
    "processed_test_data = scalling_model.transform(encoded_test_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
